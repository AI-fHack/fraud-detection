"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á 29-36 (Feature Engineering - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –∑–∞–¥–∞—á–∏ –∏–∑ notebooks/02_feature_engineering.ipynb
"""
import sys
import io
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import joblib

warnings.filterwarnings('ignore')

print("‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================================
INPUT_PATH = 'data/processed/transactions_with_features.csv'
OUTPUT_PATH = 'data/processed/transactions_with_features_final.csv'
PIPELINE_PATH = 'model/preprocessing_pipeline.pkl'

print("\n" + "="*80)
print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó –ü–†–ï–î–´–î–£–©–ï–ì–û –≠–¢–ê–ü–ê")
print("="*80)

df = pd.read_csv(INPUT_PATH, low_memory=False)
print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}")
print(f"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
print(f"–°—Ç—Ä–æ–∫: {len(df)}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
if 'target' in df.columns:
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    print(df['target'].value_counts())
    print(f"–î–∏—Å–±–∞–ª–∞–Ω—Å: {df['target'].value_counts()[0] / df['target'].value_counts()[1]:.2f}:1")
else:
    raise ValueError("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

# ============================================================================
# –ó–ê–î–ê–ß–ê 29: –û–ë–†–ê–ë–û–¢–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 29: –û–ë–†–ê–ë–û–¢–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*80)

# –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
print(f"\n–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(categorical_cols)}")
if len(categorical_cols) > 0:
    print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {categorical_cols}")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
label_encoders = {}
frequency_encodings = {}

for col in categorical_cols:
    if col == 'target' or col == 'user_id':
        continue
    
    print(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏: {col} ---")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    unique_count = df[col].nunique()
    print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_count}")
    
    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (>50) - –∏—Å–ø–æ–ª—å–∑—É–µ–º frequency encoding
    # –ï—Å–ª–∏ –º–∞–ª–æ - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å label encoding
    if unique_count > 50:
        # Frequency Encoding
        freq_map = df[col].value_counts() / len(df)
        df[f'{col}_freq_encoding'] = df[col].map(freq_map).fillna(0)
        frequency_encodings[col] = freq_map
        print(f"  ‚úÖ –°–æ–∑–¥–∞–Ω frequency encoding: {col}_freq_encoding")
        df.drop(columns=[col], inplace=True)
    elif unique_count > 2:
        # Label Encoding
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str).fillna('UNKNOWN'))
        label_encoders[col] = le
        print(f"  ‚úÖ –°–æ–∑–¥–∞–Ω label encoding: {col}_encoded")
        df.drop(columns=[col], inplace=True)
    else:
        # –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df[col] = df[col].astype(str).fillna('UNKNOWN')
        print(f"  ‚úÖ –ë–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –æ—Å—Ç–∞–≤–ª–µ–Ω –∫–∞–∫ –µ—Å—Ç—å")

print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
print(f"–°–æ–∑–¥–∞–Ω–æ label encoders: {len(label_encoders)}")
print(f"–°–æ–∑–¥–∞–Ω–æ frequency encodings: {len(frequency_encodings)}")
print(f"–¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 30: –ü–†–û–í–ï–†–ö–ê –í–´–ë–†–û–°–û–í
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 30: –ü–†–û–í–ï–†–ö–ê –í–´–ë–†–û–°–û–í")
print("="*80)

numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
if 'target' in numeric_cols:
    numeric_cols.remove('target')
if 'user_id' in numeric_cols:
    numeric_cols.remove('user_id')

print(f"\n–ü—Ä–æ–≤–µ—Ä—è–µ–º {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã...")

outliers_info = {}
for col in numeric_cols[:20]:  # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    if IQR > 0:
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        outlier_count = len(outliers)
        if outlier_count > 0:
            outliers_info[col] = {
                'count': outlier_count,
                'percentage': (outlier_count / len(df)) * 100
            }

print(f"\nüìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ {len(outliers_info)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
if len(outliers_info) > 0:
    sorted_outliers = sorted(outliers_info.items(), key=lambda x: x[1]['count'], reverse=True)[:5]
    print("–¢–æ–ø-5 –∫–æ–ª–æ–Ω–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏:")
    for col, info in sorted_outliers:
        print(f"  {col}: {info['count']} –≤—ã–±—Ä–æ—Å–æ–≤ ({info['percentage']:.2f}%)")

print("\n‚ö†Ô∏è –í–ê–ñ–ù–û: –î–ª—è fraud detection –≤—ã–±—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞")
print("–í—ã–±—Ä–æ—Å—ã –ù–ï —É–¥–∞–ª—è—é—Ç—Å—è, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

# ============================================================================
# –ó–ê–î–ê–ß–ê 31: –ù–û–†–ú–ò–†–û–í–ê–ù–ò–ï/–ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 31: –ù–û–†–ú–ò–†–û–í–ê–ù–ò–ï/–ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*80)

numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
exclude_cols = ['target', 'user_id']
exclude_patterns = ['_sin', '_cos', 'is_', '_encoded']

cols_to_scale = []
for col in numeric_cols:
    if col in exclude_cols:
        continue
    if any(pattern in col for pattern in exclude_patterns):
        continue
    unique_vals = df[col].dropna().unique()
    if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):
        continue
    cols_to_scale.append(col)

print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(cols_to_scale)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º MinMaxScaler
scaler = MinMaxScaler()
df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])

print(f"‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ {len(cols_to_scale)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é MinMaxScaler")

# ============================================================================
# –ó–ê–î–ê–ß–ê 32: –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 32: –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
print("="*80)

df.fillna(0, inplace=True)

if 'user_id' in df.columns:
    user_ids = df['user_id'].copy()
    df_for_model = df.drop(columns=['user_id'], errors='ignore')
else:
    df_for_model = df.copy()

print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:")
print(f"  –†–∞–∑–º–µ—Ä: {df_for_model.shape}")
print(f"  –ö–æ–ª–æ–Ω–æ–∫: {len(df_for_model.columns)}")
print(f"  –°—Ç—Ä–æ–∫: {len(df_for_model)}")

if 'target' not in df_for_model.columns:
    raise ValueError("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç!")

output_dir = os.path.dirname(OUTPUT_PATH)
if output_dir and not os.path.exists(output_dir):
    os.makedirs(output_dir, exist_ok=True)

df_for_model.to_csv(OUTPUT_PATH, index=False)
print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_PATH}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 34: PREPROCESSING PIPELINE
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 34: –°–û–ó–î–ê–ù–ò–ï PREPROCESSING PIPELINE")
print("="*80)

class FraudDetectionPreprocessor:
    """Preprocessing pipeline –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏."""
    
    def __init__(self, label_encoders=None, frequency_encodings=None, scaler=None, feature_columns=None):
        self.label_encoders = label_encoders or {}
        self.frequency_encodings = frequency_encodings or {}
        self.scaler = scaler
        self.feature_columns = feature_columns
        
    def fit(self, df):
        if self.feature_columns is None:
            self.feature_columns = [col for col in df.columns if col not in ['target', 'user_id']]
        return self
    
    def transform(self, df):
        df_processed = df.copy()
        
        for col, le in self.label_encoders.items():
            if col in df_processed.columns:
                df_processed[f'{col}_encoded'] = le.transform(
                    df_processed[col].astype(str).fillna('UNKNOWN')
                )
                df_processed.drop(columns=[col], inplace=True, errors='ignore')
        
        for col, freq_map in self.frequency_encodings.items():
            if col in df_processed.columns:
                df_processed[f'{col}_freq_encoding'] = df_processed[col].map(freq_map).fillna(0)
                df_processed.drop(columns=[col], inplace=True, errors='ignore')
        
        df_processed.fillna(0, inplace=True)
        
        if self.scaler is not None and self.feature_columns is not None:
            available_cols = [col for col in self.feature_columns if col in df_processed.columns]
            if available_cols:
                df_processed[available_cols] = self.scaler.transform(df_processed[available_cols])
        
        if self.feature_columns is not None:
            keep_cols = self.feature_columns.copy()
            if 'target' in df_processed.columns:
                keep_cols.append('target')
            if 'user_id' in df_processed.columns:
                keep_cols.append('user_id')
            df_processed = df_processed[[col for col in keep_cols if col in df_processed.columns]]
        
        return df_processed
    
    def fit_transform(self, df):
        return self.fit(df).transform(df)

preprocessor = FraudDetectionPreprocessor(
    label_encoders=label_encoders,
    frequency_encodings=frequency_encodings,
    scaler=scaler,
    feature_columns=[col for col in df_for_model.columns if col != 'target']
)

os.makedirs(os.path.dirname(PIPELINE_PATH), exist_ok=True)
joblib.dump(preprocessor, PIPELINE_PATH)
print(f"\n‚úÖ Preprocessing pipeline —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {PIPELINE_PATH}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 35: –§–£–ù–ö–¶–ò–Ø –í–ê–õ–ò–î–ê–¶–ò–ò
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 35: –§–£–ù–ö–¶–ò–Ø –í–ê–õ–ò–î–ê–¶–ò–ò –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•")
print("="*80)

def validate_input_data(df, required_features=None, target_col='target'):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏."""
    errors = []
    warnings = []
    
    if not isinstance(df, pd.DataFrame):
        errors.append("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å pandas.DataFrame")
        return {'is_valid': False, 'errors': errors, 'warnings': warnings}
    
    if df.empty:
        errors.append("DataFrame –ø—É—Å—Ç")
        return {'is_valid': False, 'errors': errors, 'warnings': warnings}
    
    if required_features is not None:
        missing_features = [f for f in required_features if f not in df.columns]
        if missing_features:
            errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features[:10]}")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    non_numeric_cols = [col for col in df.columns 
                       if col not in numeric_cols and col not in [target_col, 'user_id']]
    if non_numeric_cols:
        errors.append(f"–ù–∞–π–¥–µ–Ω—ã –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {non_numeric_cols[:5]}")
    
    missing_counts = df.isnull().sum()
    cols_with_missing = missing_counts[missing_counts > 0]
    if len(cols_with_missing) > 0:
        warnings.append(f"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_missing)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
    
    inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()
    cols_with_inf = inf_counts[inf_counts > 0]
    if len(cols_with_inf) > 0:
        errors.append(f"–ù–∞–π–¥–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_inf)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
    
    is_valid = len(errors) == 0
    
    return {
        'is_valid': is_valid,
        'errors': errors,
        'warnings': warnings,
        'shape': df.shape
    }

validation_result = validate_input_data(df_for_model, 
                                        required_features=[col for col in df_for_model.columns if col != 'target'])

print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
print(f"  –í–∞–ª–∏–¥–Ω–æ: {validation_result['is_valid']}")
print(f"  –û—à–∏–±–æ–∫: {len(validation_result['errors'])}")
print(f"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(validation_result['warnings'])}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏
validation_file_path = 'src/data_validation.py'
os.makedirs(os.path.dirname(validation_file_path), exist_ok=True)
with open(validation_file_path, 'w', encoding='utf-8') as f:
    f.write('''"""
–§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏.
"""
import pandas as pd
import numpy as np

def validate_input_data(df, required_features=None, target_col='target'):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏."""
    errors = []
    warnings = []
    
    if not isinstance(df, pd.DataFrame):
        errors.append("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å pandas.DataFrame")
        return {'is_valid': False, 'errors': errors, 'warnings': warnings}
    
    if df.empty:
        errors.append("DataFrame –ø—É—Å—Ç")
        return {'is_valid': False, 'errors': errors, 'warnings': warnings}
    
    if required_features is not None:
        missing_features = [f for f in required_features if f not in df.columns]
        if missing_features:
            errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features[:10]}")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    non_numeric_cols = [col for col in df.columns 
                       if col not in numeric_cols and col not in [target_col, 'user_id']]
    if non_numeric_cols:
        errors.append(f"–ù–∞–π–¥–µ–Ω—ã –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {non_numeric_cols[:5]}")
    
    missing_counts = df.isnull().sum()
    cols_with_missing = missing_counts[missing_counts > 0]
    if len(cols_with_missing) > 0:
        warnings.append(f"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_missing)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
    
    inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()
    cols_with_inf = inf_counts[inf_counts > 0]
    if len(cols_with_inf) > 0:
        errors.append(f"–ù–∞–π–¥–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_inf)} –∫–æ–ª–æ–Ω–∫–∞—Ö")
    
    is_valid = len(errors) == 0
    
    return {
        'is_valid': is_valid,
        'errors': errors,
        'warnings': warnings,
        'shape': df.shape
    }
''')

print(f"\n‚úÖ –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {validation_file_path}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 36: –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 36: –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í")
print("="*80)

final_features = [col for col in df_for_model.columns if col != 'target']

print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í:")
print(f"  –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(final_features)}")
print(f"  –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df_for_model.shape}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
features_list_path = 'model/final_features_list.txt'
with open(features_list_path, 'w', encoding='utf-8') as f:
    f.write("–§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø ML –ú–û–î–ï–õ–ò\n")
    f.write("="*80 + "\n\n")
    f.write(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(final_features)}\n\n")
    for feat in final_features:
        f.write(f"  - {feat}\n")

joblib.dump(final_features, 'model/final_features.pkl')
print(f"\n‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {features_list_path}")
print(f"‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω (pickle): model/final_features.pkl")

# ============================================================================
# –ò–¢–û–ì–ò
# ============================================================================
print("\n" + "="*80)
print("‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò 29-36 –í–´–ü–û–õ–ù–ï–ù–´!")
print("="*80)
print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê:")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 29: –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 30: –í—ã–±—Ä–æ—Å—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 31: –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 32: –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_PATH}")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 33: Notebook —Å–æ–∑–¥–∞–Ω: notebooks/02_feature_engineering.ipynb")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 34: Preprocessing pipeline —Å–æ–∑–¥–∞–Ω: {PIPELINE_PATH}")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 35: –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∞: {validation_file_path}")
print(f"  ‚úÖ –ó–∞–¥–∞—á–∞ 36: –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω: {len(final_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
print(f"\nüéØ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏!")
print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª: {OUTPUT_PATH}")

