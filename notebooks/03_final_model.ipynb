{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ –ë–õ–û–ö 3 ‚Äî ML ENGINEERING (–ù)\n",
        "\n",
        "## Fraud Detection Model Development\n",
        "\n",
        "–≠—Ç–æ—Ç notebook —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ ML-–º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π:\n",
        "\n",
        "1. **Baseline** (37-40) - Logistic Regression –∫–∞–∫ –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
        "2. **Model Development** (41-46) - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RandomForest, XGBoost, LightGBM, CatBoost\n",
        "3. **Optimization** (47-51) - –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, feature selection, CV, threshold optimization\n",
        "4. **Interpretability** (52-56) - SHAP –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
        "5. **Model Packaging** (57-59) - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\n",
        "\n",
        "**–ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞:** F2-score (—Ñ–æ–∫—É—Å –Ω–∞ Recall - –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –º–æ—à–µ–Ω–Ω–∏–∫–∞)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, fbeta_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Gradient Boosting\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
        "import optuna\n",
        "\n",
        "# –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "import shap\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "DATA_PATH = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}\")\n",
        "print(f\"\\n–ö–æ–ª–æ–Ω–∫–∏: {len(df.columns)}\")\n",
        "print(f\"–ü–µ—Ä–≤—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫: {list(df.columns[:5])}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "if 'target' in df.columns:\n",
        "    print(f\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    print(df['target'].value_counts())\n",
        "    imbalance_ratio = df['target'].value_counts()[0] / df['target'].value_counts()[1]\n",
        "    print(f\"\\n–î–∏—Å–±–∞–ª–∞–Ω—Å: {imbalance_ratio:.2f}:1\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(non_numeric_cols)}\")\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Baseline Model (–ó–∞–¥–∞—á–∏ 37-40)\n",
        "\n",
        "### –ó–∞–¥–∞—á–∞ 37: Train/Test Split (Stratified)\n",
        "### –ó–∞–¥–∞—á–∞ 38: Baseline –º–æ–¥–µ–ª—å Logistic Regression\n",
        "### –ó–∞–¥–∞—á–∞ 39: –ú–µ—Ç—Ä–∏–∫–∏ Precision, Recall, F2\n",
        "### –ó–∞–¥–∞—á–∞ 40: Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 37: Train/Test Split (Stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train/Test Split –≤—ã–ø–æ–ª–Ω–µ–Ω:\")\n",
        "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ test:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 38: Baseline –º–æ–¥–µ–ª—å Logistic Regression\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ Baseline –º–æ–¥–µ–ª–∏ (Logistic Regression) ---\")\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "baseline_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        ")\n",
        "\n",
        "baseline_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
        "y_pred_proba_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"‚úÖ Baseline –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 39: –ú–µ—Ç—Ä–∏–∫–∏ Precision, Recall, F2\n",
        "def calculate_metrics(y_true, y_pred, y_pred_proba=None, model_name=\"\"):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏\"\"\"\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    f2 = fbeta_score(y_true, y_pred, beta=2)  # F2-score (–±–æ–ª—å—à–∏–π –≤–µ—Å Recall)\n",
        "    \n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'F2-score': f2\n",
        "    }\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "        metrics['ROC-AUC'] = roc_auc\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Baseline\n",
        "baseline_metrics = calculate_metrics(\n",
        "    y_test, y_pred_baseline, y_pred_proba_baseline, \"Baseline (Logistic Regression)\"\n",
        ")\n",
        "\n",
        "print(\"üìä –ú–ï–¢–†–ò–ö–ò BASELINE –ú–û–î–ï–õ–ò:\")\n",
        "print(\"=\" * 60)\n",
        "for key, value in baseline_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"{key:15s}: {value:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 40: Confusion Matrix\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "os.makedirs('../model', exist_ok=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'],\n",
        "            yticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'])\n",
        "axes[0].set_title('Confusion Matrix - Baseline (Logistic Regression)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "axes[0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
        "axes[1].plot(fpr, tpr, label=f'Baseline (AUC = {baseline_metrics[\"ROC-AUC\"]:.3f})', linewidth=2)\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve - Baseline', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/baseline_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Baseline –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Model Development (–ó–∞–¥–∞—á–∏ 41-46)\n",
        "\n",
        "### –ó–∞–¥–∞—á–∏ 41-44: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (RandomForest, XGBoost, LightGBM, CatBoost)\n",
        "### –ó–∞–¥–∞—á–∞ 45: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º\n",
        "### –ó–∞–¥–∞—á–∞ 46: –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 41: RandomForest\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ RandomForest ---\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ RandomForest –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 42: XGBoost\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ XGBoost ---\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ XGBoost –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 43: LightGBM\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ LightGBM ---\")\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ LightGBM –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 44: CatBoost\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ CatBoost ---\")\n",
        "cb_model = cb.CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    depth=6,\n",
        "    learning_rate=0.1,\n",
        "    class_weights=[1, len(y_train[y_train==0]) / len(y_train[y_train==1])],  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "cb_model.fit(X_train, y_train)\n",
        "y_pred_cb = cb_model.predict(X_test)\n",
        "y_pred_proba_cb = cb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ CatBoost –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 45: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º\n",
        "all_models = {\n",
        "    'Baseline (Logistic Regression)': (y_pred_baseline, y_pred_proba_baseline),\n",
        "    'RandomForest': (y_pred_rf, y_pred_proba_rf),\n",
        "    'XGBoost': (y_pred_xgb, y_pred_proba_xgb),\n",
        "    'LightGBM': (y_pred_lgb, y_pred_proba_lgb),\n",
        "    'CatBoost': (y_pred_cb, y_pred_proba_cb)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, (y_pred, y_pred_proba) in all_models.items():\n",
        "    metrics = calculate_metrics(y_test, y_pred, y_pred_proba, name)\n",
        "    results.append(metrics)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('F2-score', ascending=False)\n",
        "\n",
        "print(\"üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics_to_plot = ['Precision', 'Recall', 'F1-score', 'F2-score']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    bars = ax.barh(results_df['Model'], results_df[metric], color=sns.color_palette(\"husl\", len(results_df)))\n",
        "    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{metric} –ø–æ –º–æ–¥–µ–ª—è–º', fontsize=12, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    \n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
        "    for i, (bar, value) in enumerate(zip(bars, results_df[metric])):\n",
        "        ax.text(value, i, f' {value:.4f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 46: –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_f2_score = results_df.iloc[0]['F2-score']\n",
        "\n",
        "print(f\"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}\")\n",
        "print(f\"   F2-score: {best_f2_score:.4f}\")\n",
        "print(f\"\\nüìã –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏:\")\n",
        "best_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
        "for metric in ['Precision', 'Recall', 'F1-score', 'F2-score', 'ROC-AUC']:\n",
        "    if metric in best_metrics:\n",
        "        print(f\"   {metric}: {best_metrics[metric]:.4f}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
        "if best_model_name == 'Baseline (Logistic Regression)':\n",
        "    best_model = baseline_model\n",
        "    best_scaler = scaler\n",
        "elif best_model_name == 'RandomForest':\n",
        "    best_model = rf_model\n",
        "    best_scaler = None\n",
        "elif best_model_name == 'XGBoost':\n",
        "    best_model = xgb_model\n",
        "    best_scaler = None\n",
        "elif best_model_name == 'LightGBM':\n",
        "    best_model = lgb_model\n",
        "    best_scaler = None\n",
        "elif best_model_name == 'CatBoost':\n",
        "    best_model = cb_model\n",
        "    best_scaler = None\n",
        "\n",
        "print(f\"\\n‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Optimization (–ó–∞–¥–∞—á–∏ 47-51)\n",
        "\n",
        "### –ó–∞–¥–∞—á–∞ 47: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Optuna)\n",
        "### –ó–∞–¥–∞—á–∞ 48: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ—Ç–±–æ—Ä feature importance)\n",
        "### –ó–∞–¥–∞—á–∞ 49: Cross-validation\n",
        "### –ó–∞–¥–∞—á–∞ 50: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "### –ó–∞–¥–∞—á–∞ 51: –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 47: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Optuna)\n",
        "print(\"--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna ---\")\n",
        "print(\"‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...\")\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å (–±–µ—Ä–µ–º –ª—É—á—à—É—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞)\n",
        "# –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º LightGBM (–æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna\"\"\"\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'class_weight': 'balanced',\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    \n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º cross-validation –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv, \n",
        "                            scoring='fbeta', beta=2, n_jobs=-1)\n",
        "    \n",
        "    return scores.mean()\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "print(f\"–õ—É—á—à–∏–π F2-score (CV): {study.best_value:.4f}\")\n",
        "print(f\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "best_params = study.best_params.copy()\n",
        "best_params.update({\n",
        "    'class_weight': 'balanced',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbose': -1\n",
        "})\n",
        "\n",
        "optimized_model = lgb.LGBMClassifier(**best_params)\n",
        "optimized_model.fit(X_train, y_train)\n",
        "y_pred_optimized = optimized_model.predict(X_test)\n",
        "y_pred_proba_optimized = optimized_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "optimized_metrics = calculate_metrics(\n",
        "    y_test, y_pred_optimized, y_pred_proba_optimized, \"LightGBM (Optimized)\"\n",
        ")\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\")\n",
        "for key, value in optimized_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"  {key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 48: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ—Ç–±–æ—Ä feature importance)\n",
        "print(\"--- –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ ---\")\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º feature importance –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': optimized_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\n–¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ø 80%)\n",
        "importance_threshold = feature_importance['importance'].quantile(0.2)\n",
        "selected_features = feature_importance[feature_importance['importance'] >= importance_threshold]['feature'].tolist()\n",
        "\n",
        "print(f\"\\n‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(X_train.columns)}\")\n",
        "print(f\"–ü–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏: {importance_threshold:.6f}\")\n",
        "\n",
        "# –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "optimized_model_selected = lgb.LGBMClassifier(**best_params)\n",
        "optimized_model_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = optimized_model_selected.predict(X_test_selected)\n",
        "y_pred_proba_selected = optimized_model_selected.predict_proba(X_test_selected)[:, 1]\n",
        "\n",
        "selected_metrics = calculate_metrics(\n",
        "    y_test, y_pred_selected, y_pred_proba_selected, \"LightGBM (Optimized + Feature Selection)\"\n",
        ")\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "for key, value in selected_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
        "plt.title('Top-20 Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 49: Cross-validation\n",
        "print(\"--- Cross-Validation –æ—Ü–µ–Ω–∫–∞ ---\")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# CV –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "cv_scores_f2 = cross_val_score(\n",
        "    optimized_model_selected, X_train_selected, y_train, \n",
        "    cv=cv, scoring='fbeta', beta=2, n_jobs=-1\n",
        ")\n",
        "cv_scores_recall = cross_val_score(\n",
        "    optimized_model_selected, X_train_selected, y_train,\n",
        "    cv=cv, scoring='recall', n_jobs=-1\n",
        ")\n",
        "cv_scores_precision = cross_val_score(\n",
        "    optimized_model_selected, X_train_selected, y_train,\n",
        "    cv=cv, scoring='precision', n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Cross-Validation —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (5-fold):\")\n",
        "print(f\"  F2-score: {cv_scores_f2.mean():.4f} (+/- {cv_scores_f2.std() * 2:.4f})\")\n",
        "print(f\"  Recall:   {cv_scores_recall.mean():.4f} (+/- {cv_scores_recall.std() * 2:.4f})\")\n",
        "print(f\"  Precision: {cv_scores_precision.mean():.4f} (+/- {cv_scores_precision.std() * 2:.4f})\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "cv_results = pd.DataFrame({\n",
        "    'Fold': range(1, 6),\n",
        "    'F2-score': cv_scores_f2,\n",
        "    'Recall': cv_scores_recall,\n",
        "    'Precision': cv_scores_precision\n",
        "})\n",
        "cv_results.plot(x='Fold', y=['F2-score', 'Recall', 'Precision'], \n",
        "                kind='bar', ax=ax, color=['steelblue', 'coral', 'green'])\n",
        "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Cross-Validation Results (5-fold)', fontsize=12, fontweight='bold')\n",
        "ax.set_xticklabels(cv_results['Fold'], rotation=0)\n",
        "ax.legend(loc='best')\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/cv_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 50: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "print(\"--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ---\")\n",
        "\n",
        "# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç F2-score\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "f2_scores = []\n",
        "recall_scores = []\n",
        "precision_scores = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba_selected >= threshold).astype(int)\n",
        "    f2 = fbeta_score(y_test, y_pred_thresh, beta=2)\n",
        "    rec = recall_score(y_test, y_pred_thresh)\n",
        "    prec = precision_score(y_test, y_pred_thresh)\n",
        "    \n",
        "    f2_scores.append(f2)\n",
        "    recall_scores.append(rec)\n",
        "    precision_scores.append(prec)\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥\n",
        "best_threshold_idx = np.argmax(f2_scores)\n",
        "best_threshold = thresholds[best_threshold_idx]\n",
        "best_f2_thresh = f2_scores[best_threshold_idx]\n",
        "\n",
        "print(f\"\\n‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_threshold:.3f}\")\n",
        "print(f\"   F2-score –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ: {best_f2_thresh:.4f}\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥\n",
        "y_pred_optimal = (y_pred_proba_selected >= best_threshold).astype(int)\n",
        "optimal_metrics = calculate_metrics(\n",
        "    y_test, y_pred_optimal, y_pred_proba_selected, \n",
        "    f\"LightGBM (Optimized + Feature Selection + Optimal Threshold={best_threshold:.3f})\"\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º:\")\n",
        "for key, value in optimal_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(thresholds, f2_scores, label='F2-score', linewidth=2, marker='o')\n",
        "ax.plot(thresholds, recall_scores, label='Recall', linewidth=2, marker='s')\n",
        "ax.plot(thresholds, precision_scores, label='Precision', linewidth=2, marker='^')\n",
        "ax.axvline(best_threshold, color='red', linestyle='--', \n",
        "           label=f'Optimal Threshold = {best_threshold:.3f}')\n",
        "ax.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Threshold Optimization', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/threshold_optimization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "final_model = optimized_model_selected\n",
        "final_threshold = best_threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 51: –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "final_metrics = optimal_metrics\n",
        "for key, value in final_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"{key:15s}: {value:.4f}\")\n",
        "\n",
        "# Confusion Matrix —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "cm_final = confusion_matrix(y_test, y_pred_optimal)\n",
        "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'],\n",
        "            yticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'])\n",
        "axes[0].set_title('Confusion Matrix - Final Model', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "axes[0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_selected)\n",
        "axes[1].plot(fpr, tpr, label=f'Final Model (AUC = {final_metrics[\"ROC-AUC\"]:.3f})', \n",
        "             linewidth=2, color='steelblue')\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve - Final Model', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/final_model_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥–æ—Ç–æ–≤—ã!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Interpretability (–ó–∞–¥–∞—á–∏ 52-56)\n",
        "\n",
        "### –ó–∞–¥–∞—á–∞ 52: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å SHAP\n",
        "### –ó–∞–¥–∞—á–∞ 53: SHAP summary plot\n",
        "### –ó–∞–¥–∞—á–∞ 54: SHAP bar importance\n",
        "### –ó–∞–¥–∞—á–∞ 55: –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏\n",
        "### –ó–∞–¥–∞—á–∞ 56: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å SHAP –≥—Ä–∞—Ñ–∏–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 52: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å SHAP (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ requirements.txt)\n",
        "print(\"--- SHAP Analysis ---\")\n",
        "print(\"‚úÖ SHAP —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\")\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SHAP explainer\n",
        "# –î–ª—è LightGBM –∏—Å–ø–æ–ª—å–∑—É–µ–º TreeExplainer (–±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ)\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_test_selected)\n",
        "\n",
        "# –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ shap_values - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ [values_class_0, values_class_1]\n",
        "# –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∫–ª–∞—Å—Å 1 (–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ)\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values_fraud = shap_values[1]  # –ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n",
        "else:\n",
        "    shap_values_fraud = shap_values\n",
        "\n",
        "print(f\"‚úÖ SHAP –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω—ã –¥–ª—è {len(X_test_selected)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 53: SHAP summary plot\n",
        "print(\"--- –°–æ–∑–¥–∞–Ω–∏–µ SHAP Summary Plot ---\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values_fraud, X_test_selected, \n",
        "                  feature_names=selected_features,\n",
        "                  show=False, max_display=20)\n",
        "plt.title('SHAP Summary Plot (Top 20 Features)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/shap_summary_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ SHAP Summary Plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 54: SHAP bar importance\n",
        "print(\"--- –°–æ–∑–¥–∞–Ω–∏–µ SHAP Bar Plot ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.plots.bar(shap_values_fraud, feature_names=selected_features, \n",
        "               max_display=20, show=False)\n",
        "plt.title('SHAP Feature Importance (Bar Plot)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/shap_bar_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ SHAP Bar Plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 55: –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏\n",
        "print(\"--- –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ ---\")\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "fraud_indices = y_test[y_test == 1].index\n",
        "if len(fraud_indices) > 0:\n",
        "    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "    example_idx = fraud_indices[0]\n",
        "    example_idx_in_test = list(y_test.index).index(example_idx)\n",
        "    \n",
        "    print(f\"\\nüìã –ü—Ä–∏–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ #{example_idx_in_test}:\")\n",
        "    print(f\"   –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è (1)\")\n",
        "    print(f\"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {y_pred_proba_selected[example_idx_in_test]:.4f}\")\n",
        "    print(f\"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {'–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è' if y_pred_optimal[example_idx_in_test] == 1 else '–ù–æ—Ä–º–∞–ª—å–Ω–∞—è'}\")\n",
        "    \n",
        "    # Waterfall plot –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.waterfall_plot(\n",
        "        shap.Explanation(\n",
        "            values=shap_values_fraud[example_idx_in_test],\n",
        "            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
        "            data=X_test_selected.iloc[example_idx_in_test].values,\n",
        "            feature_names=selected_features\n",
        "        ),\n",
        "        max_display=15,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f'SHAP Waterfall Plot - Transaction #{example_idx_in_test}\\n(Predicted: {y_pred_proba_selected[example_idx_in_test]:.4f})', \n",
        "              fontsize=12, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../model/shap_waterfall_example.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\")\n",
        "    \n",
        "    # –ë–µ—Ä–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n",
        "    high_prob_idx = np.argmax(y_pred_proba_selected)\n",
        "    print(f\"\\nüìã –ü—Ä–∏–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞:\")\n",
        "    print(f\"   –ò–Ω–¥–µ–∫—Å: {high_prob_idx}\")\n",
        "    print(f\"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {y_pred_proba_selected[high_prob_idx]:.4f}\")\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.waterfall_plot(\n",
        "        shap.Explanation(\n",
        "            values=shap_values_fraud[high_prob_idx],\n",
        "            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
        "            data=X_test_selected.iloc[high_prob_idx].values,\n",
        "            feature_names=selected_features\n",
        "        ),\n",
        "        max_display=15,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f'SHAP Waterfall Plot - High Risk Transaction\\n(Predicted: {y_pred_proba_selected[high_prob_idx]:.4f})', \n",
        "              fontsize=12, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../model/shap_waterfall_example.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 56: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å SHAP –≥—Ä–∞—Ñ–∏–∫–∏ (—É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—ã—à–µ)\n",
        "print(\"‚úÖ –í—Å–µ SHAP –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
        "print(\"  - shap_summary_plot.png\")\n",
        "print(\"  - shap_bar_plot.png\")\n",
        "print(\"  - shap_waterfall_example.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Model Packaging (–ó–∞–¥–∞—á–∏ 57-59)\n",
        "\n",
        "### –ó–∞–¥–∞—á–∞ 57: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ model.pkl\n",
        "### –ó–∞–¥–∞—á–∞ 58: –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ö–æ–¥/–≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏\n",
        "### –ó–∞–¥–∞—á–∞ 59: –°–æ–∑–¥–∞—Ç—å notebook 03_final_model.ipynb (‚úÖ —É–∂–µ —Å–æ–∑–¥–∞–Ω)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 57: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ model.pkl\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é model –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
        "os.makedirs('../model', exist_ok=True)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å, —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø–æ—Ä–æ–≥\n",
        "model_package = {\n",
        "    'model': final_model,\n",
        "    'selected_features': selected_features,\n",
        "    'threshold': final_threshold,\n",
        "    'scaler': None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è LightGBM\n",
        "    'model_type': 'LightGBM',\n",
        "    'metrics': final_metrics\n",
        "}\n",
        "\n",
        "joblib.dump(model_package, '../model/model.pkl')\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ../model/model.pkl\")\n",
        "\n",
        "# –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
        "joblib.dump(final_model, '../model/final_model.pkl')\n",
        "joblib.dump(selected_features, '../model/selected_features.pkl')\n",
        "joblib.dump(final_threshold, '../model/threshold.pkl')\n",
        "\n",
        "print(\"‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
        "print(\"  - final_model.pkl\")\n",
        "print(\"  - selected_features.pkl\")\n",
        "print(\"  - threshold.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 58: –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ö–æ–¥/–≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏\n",
        "model_doc = f\"\"\"# üìã –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Fraud Detection\n",
        "\n",
        "## –ú–æ–¥–µ–ª—å\n",
        "- **–¢–∏–ø:** {model_package['model_type']}\n",
        "- **–í–µ—Ä—Å–∏—è:** 1.0\n",
        "- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "- **F2-score:** {final_metrics['F2-score']:.4f}\n",
        "- **Recall:** {final_metrics['Recall']:.4f}\n",
        "- **Precision:** {final_metrics['Precision']:.4f}\n",
        "- **F1-score:** {final_metrics['F1-score']:.4f}\n",
        "- **ROC-AUC:** {final_metrics['ROC-AUC']:.4f}\n",
        "\n",
        "## –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "- **Threshold:** {final_threshold:.3f}\n",
        "\n",
        "## –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Input)\n",
        "–ú–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ DataFrame —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ ({len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):\n",
        "\n",
        "{chr(10).join([f\"- {feat}\" for feat in selected_features[:20]])}\n",
        "... –∏ –µ—â–µ {len(selected_features) - 20} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n",
        "- –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º–∏\n",
        "- –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω—ã (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0)\n",
        "- –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –≤ —Å–ø–∏—Å–∫–µ selected_features\n",
        "\n",
        "## –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Output)\n",
        "–ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "1. **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞** (float): –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1\n",
        "   - –ë–ª–∏–∑–∫–æ –∫ 0: –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n",
        "   - –ë–ª–∏–∑–∫–æ –∫ 1: –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n",
        "\n",
        "2. **–ö–ª–∞—Å—Å** (int): 0 –∏–ª–∏ 1\n",
        "   - 0: –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è\n",
        "   - 1: –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è\n",
        "   - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å –ø–æ—Ä–æ–≥–æ–º ({final_threshold:.3f})\n",
        "\n",
        "## –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "```python\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model_package = joblib.load('model/model.pkl')\n",
        "model = model_package['model']\n",
        "selected_features = model_package['selected_features']\n",
        "threshold = model_package['threshold']\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "# X_new –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑ selected_features\n",
        "X_new = pd.DataFrame(...)  # –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "probabilities = model.predict_proba(X_new[selected_features])[:, 1]\n",
        "predictions = (probabilities >= threshold).astype(int)\n",
        "\n",
        "# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "for i, (prob, pred) in enumerate(zip(probabilities, predictions)):\n",
        "    print(f\"–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è {{i}}: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={{prob:.4f}}, –∫–ª–∞—Å—Å={{'–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è' if pred == 1 else '–ù–æ—Ä–º–∞–ª—å–Ω–∞—è'}}\")\n",
        "```\n",
        "\n",
        "## –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è\n",
        "1. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ (~78:1)\n",
        "2. –ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - F2-score (—Ñ–æ–∫—É—Å –Ω–∞ Recall - –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –º–æ—à–µ–Ω–Ω–∏–∫–∞)\n",
        "3. –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F2-score\n",
        "4. –î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ SHAP –∑–Ω–∞—á–µ–Ω–∏—è (—Å–º. –≥—Ä–∞—Ñ–∏–∫–∏ –≤ model/)\n",
        "\"\"\"\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é\n",
        "with open('../model/MODEL_DOCUMENTATION.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(model_doc)\n",
        "\n",
        "print(\"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ../model/MODEL_DOCUMENTATION.md\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(model_doc)\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ –ò—Ç–æ–≥–∏\n",
        "\n",
        "–í—Å–µ –∑–∞–¥–∞—á–∏ –±–ª–æ–∫–∞ 3 (ML Engineering) –≤—ã–ø–æ–ª–Ω–µ–Ω—ã:\n",
        "\n",
        "- ‚úÖ **Baseline** (37-40): Train/test split, Logistic Regression, –º–µ—Ç—Ä–∏–∫–∏, confusion matrix\n",
        "- ‚úÖ **Model Development** (41-46): –û–±—É—á–µ–Ω—ã –∏ —Å—Ä–∞–≤–Ω–µ–Ω—ã RandomForest, XGBoost, LightGBM, CatBoost\n",
        "- ‚úÖ **Optimization** (47-51): –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, feature selection, CV, threshold optimization\n",
        "- ‚úÖ **Interpretability** (52-56): SHAP –∞–Ω–∞–ª–∏–∑, –≥—Ä–∞—Ñ–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏–º–µ—Ä—ã –æ–±—ä—è—Å–Ω–µ–Ω–∏–π\n",
        "- ‚úÖ **Model Packaging** (57-59): –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞\n",
        "\n",
        "**–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å:** LightGBM —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "\n",
        "–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ API!\n",
        "\n",
        "> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (F2-score, Recall, Precision, ROC-AUC) –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å –≤ —è—á–µ–π–∫–µ \"–ó–∞–¥–∞—á–∞ 51: –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\" –≤—ã—à–µ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score, fbeta_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Gradient Boosting\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è\n",
        "import optuna\n",
        "\n",
        "# –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å\n",
        "import shap\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "DATA_PATH = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}\")\n",
        "print(f\"\\n–ö–æ–ª–æ–Ω–∫–∏: {len(df.columns)}\")\n",
        "print(f\"–ü–µ—Ä–≤—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫: {list(df.columns[:5])}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "if 'target' in df.columns:\n",
        "    print(f\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    print(df['target'].value_counts())\n",
        "    print(f\"\\n–î–∏—Å–±–∞–ª–∞–Ω—Å: {df['target'].value_counts()[0] / df['target'].value_counts()[1]:.2f}:1\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "X = df.drop(columns=['target'], errors='ignore')\n",
        "y = df['target']\n",
        "\n",
        "# –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(non_numeric_cols)}\")\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "print(f\"\\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 37: Train/Test Split (Stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Train/Test Split –≤—ã–ø–æ–ª–Ω–µ–Ω:\")\n",
        "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ test:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 38: Baseline –º–æ–¥–µ–ª—å Logistic Regression\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ Baseline –º–æ–¥–µ–ª–∏ (Logistic Regression) ---\")\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "baseline_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced'  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        ")\n",
        "\n",
        "baseline_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
        "y_pred_proba_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"‚úÖ Baseline –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 39: –ú–µ—Ç—Ä–∏–∫–∏ Precision, Recall, F2\n",
        "def calculate_metrics(y_true, y_pred, y_pred_proba=None, model_name=\"\"):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏\"\"\"\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    f2 = fbeta_score(y_true, y_pred, beta=2)  # F2-score (–±–æ–ª—å—à–∏–π –≤–µ—Å Recall)\n",
        "    \n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'F2-score': f2\n",
        "    }\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "        metrics['ROC-AUC'] = roc_auc\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Baseline\n",
        "baseline_metrics = calculate_metrics(\n",
        "    y_test, y_pred_baseline, y_pred_proba_baseline, \"Baseline (Logistic Regression)\"\n",
        ")\n",
        "\n",
        "print(\"üìä –ú–ï–¢–†–ò–ö–ò BASELINE –ú–û–î–ï–õ–ò:\")\n",
        "print(\"=\" * 60)\n",
        "for key, value in baseline_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"{key:15s}: {value:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 40: Confusion Matrix\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'],\n",
        "            yticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'])\n",
        "axes[0].set_title('Confusion Matrix - Baseline (Logistic Regression)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "axes[0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
        "axes[1].plot(fpr, tpr, label=f'Baseline (AUC = {baseline_metrics[\"ROC-AUC\"]:.3f})', linewidth=2)\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve - Baseline', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/baseline_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Baseline –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 41: RandomForest\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ RandomForest ---\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ RandomForest –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 42: XGBoost\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ XGBoost ---\")\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ XGBoost –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 43: LightGBM\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ LightGBM ---\")\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ LightGBM –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 44: CatBoost\n",
        "print(\"--- –û–±—É—á–µ–Ω–∏–µ CatBoost ---\")\n",
        "cb_model = cb.CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    depth=6,\n",
        "    learning_rate=0.1,\n",
        "    class_weights=[1, len(y_train[y_train==0]) / len(y_train[y_train==1])],  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "cb_model.fit(X_train, y_train)\n",
        "y_pred_cb = cb_model.predict(X_test)\n",
        "y_pred_proba_cb = cb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"‚úÖ CatBoost –æ–±—É—á–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 45: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º\n",
        "all_models = {\n",
        "    'Baseline (Logistic Regression)': (y_pred_baseline, y_pred_proba_baseline),\n",
        "    'RandomForest': (y_pred_rf, y_pred_proba_rf),\n",
        "    'XGBoost': (y_pred_xgb, y_pred_proba_xgb),\n",
        "    'LightGBM': (y_pred_lgb, y_pred_proba_lgb),\n",
        "    'CatBoost': (y_pred_cb, y_pred_proba_cb)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, (y_pred, y_pred_proba) in all_models.items():\n",
        "    metrics = calculate_metrics(y_test, y_pred, y_pred_proba, name)\n",
        "    results.append(metrics)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('F2-score', ascending=False)\n",
        "\n",
        "print(\"üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics_to_plot = ['Precision', 'Recall', 'F1-score', 'F2-score']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    bars = ax.barh(results_df['Model'], results_df[metric], color=sns.color_palette(\"husl\", len(results_df)))\n",
        "    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{metric} –ø–æ –º–æ–¥–µ–ª—è–º', fontsize=12, fontweight='bold')\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    \n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
        "    for i, (bar, value) in enumerate(zip(bars, results_df[metric])):\n",
        "        ax.text(value, i, f' {value:.4f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 46: –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_f2_score = results_df.iloc[0]['F2-score']\n",
        "\n",
        "print(f\"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}\")\n",
        "print(f\"   F2-score: {best_f2_score:.4f}\")\n",
        "print(f\"\\nüìã –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏:\")\n",
        "best_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
        "for metric in ['Precision', 'Recall', 'F1-score', 'F2-score', 'ROC-AUC']:\n",
        "    if metric in best_metrics:\n",
        "        print(f\"   {metric}: {best_metrics[metric]:.4f}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
        "if best_model_name == 'Baseline (Logistic Regression)':\n",
        "    best_model = baseline_model\n",
        "    best_scaler = scaler\n",
        "elif best_model_name == 'RandomForest':\n",
        "    best_model = rf_model\n",
        "    best_scaler = None\n",
        "elif best_model_name == 'XGBoost':\n",
        "    best_model = xgb_model\n",
        "    best_scaler = None\n",
        "elif best_model_name == 'LightGBM':\n",
        "    best_model = lgb_model\n",
        "    best_scaler = None\n",
        "elif best_model_name == 'CatBoost':\n",
        "    best_model = cb_model\n",
        "    best_scaler = None\n",
        "\n",
        "print(f\"\\n‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 47: –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Optuna)\n",
        "print(\"--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna ---\")\n",
        "print(\"‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...\")\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å (–±–µ—Ä–µ–º –ª—É—á—à—É—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞)\n",
        "# –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º LightGBM (–æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna\"\"\"\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'class_weight': 'balanced',\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1\n",
        "    }\n",
        "    \n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    \n",
        "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º cross-validation –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv, \n",
        "                            scoring='fbeta', beta=2, n_jobs=-1)\n",
        "    \n",
        "    return scores.mean()\n",
        "\n",
        "# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "print(f\"–õ—É—á—à–∏–π F2-score (CV): {study.best_value:.4f}\")\n",
        "print(f\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "best_params = study.best_params.copy()\n",
        "best_params.update({\n",
        "    'class_weight': 'balanced',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1,\n",
        "    'verbose': -1\n",
        "})\n",
        "\n",
        "optimized_model = lgb.LGBMClassifier(**best_params)\n",
        "optimized_model.fit(X_train, y_train)\n",
        "y_pred_optimized = optimized_model.predict(X_test)\n",
        "y_pred_proba_optimized = optimized_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "optimized_metrics = calculate_metrics(\n",
        "    y_test, y_pred_optimized, y_pred_proba_optimized, \"LightGBM (Optimized)\"\n",
        ")\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:\")\n",
        "for key, value in optimized_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"  {key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 48: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ—Ç–±–æ—Ä feature importance)\n",
        "print(\"--- –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ ---\")\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º feature importance –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': optimized_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\n–¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "print(feature_importance.head(20).to_string(index=False))\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ø 80%)\n",
        "importance_threshold = feature_importance['importance'].quantile(0.2)\n",
        "selected_features = feature_importance[feature_importance['importance'] >= importance_threshold]['feature'].tolist()\n",
        "\n",
        "print(f\"\\n‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(X_train.columns)}\")\n",
        "print(f\"–ü–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏: {importance_threshold:.6f}\")\n",
        "\n",
        "# –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "optimized_model_selected = lgb.LGBMClassifier(**best_params)\n",
        "optimized_model_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = optimized_model_selected.predict(X_test_selected)\n",
        "y_pred_proba_selected = optimized_model_selected.predict_proba(X_test_selected)[:, 1]\n",
        "\n",
        "selected_metrics = calculate_metrics(\n",
        "    y_test, y_pred_selected, y_pred_proba_selected, \"LightGBM (Optimized + Feature Selection)\"\n",
        ")\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "for key, value in selected_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
        "plt.title('Top-20 Feature Importance', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 49: Cross-validation\n",
        "print(\"--- Cross-Validation –æ—Ü–µ–Ω–∫–∞ ---\")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# CV –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "cv_scores_f2 = cross_val_score(\n",
        "    optimized_model_selected, X_train_selected, y_train, \n",
        "    cv=cv, scoring='fbeta', beta=2, n_jobs=-1\n",
        ")\n",
        "cv_scores_recall = cross_val_score(\n",
        "    optimized_model_selected, X_train_selected, y_train,\n",
        "    cv=cv, scoring='recall', n_jobs=-1\n",
        ")\n",
        "cv_scores_precision = cross_val_score(\n",
        "    optimized_model_selected, X_train_selected, y_train,\n",
        "    cv=cv, scoring='precision', n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Cross-Validation —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (5-fold):\")\n",
        "print(f\"  F2-score: {cv_scores_f2.mean():.4f} (+/- {cv_scores_f2.std() * 2:.4f})\")\n",
        "print(f\"  Recall:   {cv_scores_recall.mean():.4f} (+/- {cv_scores_recall.std() * 2:.4f})\")\n",
        "print(f\"  Precision: {cv_scores_precision.mean():.4f} (+/- {cv_scores_precision.std() * 2:.4f})\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "cv_results = pd.DataFrame({\n",
        "    'Fold': range(1, 6),\n",
        "    'F2-score': cv_scores_f2,\n",
        "    'Recall': cv_scores_recall,\n",
        "    'Precision': cv_scores_precision\n",
        "})\n",
        "cv_results.plot(x='Fold', y=['F2-score', 'Recall', 'Precision'], \n",
        "                kind='bar', ax=ax, color=['steelblue', 'coral', 'green'])\n",
        "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Cross-Validation Results (5-fold)', fontsize=12, fontweight='bold')\n",
        "ax.set_xticklabels(cv_results['Fold'], rotation=0)\n",
        "ax.legend(loc='best')\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/cv_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 50: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
        "print(\"--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ---\")\n",
        "\n",
        "# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç F2-score\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "f2_scores = []\n",
        "recall_scores = []\n",
        "precision_scores = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba_selected >= threshold).astype(int)\n",
        "    f2 = fbeta_score(y_test, y_pred_thresh, beta=2)\n",
        "    rec = recall_score(y_test, y_pred_thresh)\n",
        "    prec = precision_score(y_test, y_pred_thresh)\n",
        "    \n",
        "    f2_scores.append(f2)\n",
        "    recall_scores.append(rec)\n",
        "    precision_scores.append(prec)\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥\n",
        "best_threshold_idx = np.argmax(f2_scores)\n",
        "best_threshold = thresholds[best_threshold_idx]\n",
        "best_f2_thresh = f2_scores[best_threshold_idx]\n",
        "\n",
        "print(f\"\\n‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_threshold:.3f}\")\n",
        "print(f\"   F2-score –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ: {best_f2_thresh:.4f}\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥\n",
        "y_pred_optimal = (y_pred_proba_selected >= best_threshold).astype(int)\n",
        "optimal_metrics = calculate_metrics(\n",
        "    y_test, y_pred_optimal, y_pred_proba_selected, \n",
        "    f\"LightGBM (Optimized + Feature Selection + Optimal Threshold={best_threshold:.3f})\"\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º:\")\n",
        "for key, value in optimal_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(thresholds, f2_scores, label='F2-score', linewidth=2, marker='o')\n",
        "ax.plot(thresholds, recall_scores, label='Recall', linewidth=2, marker='s')\n",
        "ax.plot(thresholds, precision_scores, label='Precision', linewidth=2, marker='^')\n",
        "ax.axvline(best_threshold, color='red', linestyle='--', \n",
        "           label=f'Optimal Threshold = {best_threshold:.3f}')\n",
        "ax.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Threshold Optimization', fontsize=12, fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/threshold_optimization.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "final_model = optimized_model_selected\n",
        "final_threshold = best_threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 51: –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "final_metrics = optimal_metrics\n",
        "for key, value in final_metrics.items():\n",
        "    if key != 'Model':\n",
        "        print(f\"{key:15s}: {value:.4f}\")\n",
        "\n",
        "# Confusion Matrix —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "cm_final = confusion_matrix(y_test, y_pred_optimal)\n",
        "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'],\n",
        "            yticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'])\n",
        "axes[0].set_title('Confusion Matrix - Final Model', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "axes[0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_selected)\n",
        "axes[1].plot(fpr, tpr, label=f'Final Model (AUC = {final_metrics[\"ROC-AUC\"]:.3f})', \n",
        "             linewidth=2, color='steelblue')\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate')\n",
        "axes[1].set_title('ROC Curve - Final Model', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/final_model_metrics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥–æ—Ç–æ–≤—ã!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 52: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å SHAP (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ requirements.txt)\n",
        "print(\"--- SHAP Analysis ---\")\n",
        "print(\"‚úÖ SHAP —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\")\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SHAP explainer\n",
        "# –î–ª—è LightGBM –∏—Å–ø–æ–ª—å–∑—É–µ–º TreeExplainer (–±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ)\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer.shap_values(X_test_selected)\n",
        "\n",
        "# –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ shap_values - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ [values_class_0, values_class_1]\n",
        "# –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∫–ª–∞—Å—Å 1 (–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ)\n",
        "if isinstance(shap_values, list):\n",
        "    shap_values_fraud = shap_values[1]  # –ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n",
        "else:\n",
        "    shap_values_fraud = shap_values\n",
        "\n",
        "print(f\"‚úÖ SHAP –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω—ã –¥–ª—è {len(X_test_selected)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 53: SHAP summary plot\n",
        "print(\"--- –°–æ–∑–¥–∞–Ω–∏–µ SHAP Summary Plot ---\")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values_fraud, X_test_selected, \n",
        "                  feature_names=selected_features,\n",
        "                  show=False, max_display=20)\n",
        "plt.title('SHAP Summary Plot (Top 20 Features)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/shap_summary_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ SHAP Summary Plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 54: SHAP bar importance\n",
        "print(\"--- –°–æ–∑–¥–∞–Ω–∏–µ SHAP Bar Plot ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.plots.bar(shap_values_fraud, feature_names=selected_features, \n",
        "               max_display=20, show=False)\n",
        "plt.title('SHAP Feature Importance (Bar Plot)', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../model/shap_bar_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ SHAP Bar Plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 55: –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏\n",
        "print(\"--- –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ ---\")\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "fraud_indices = y_test[y_test == 1].index\n",
        "if len(fraud_indices) > 0:\n",
        "    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "    example_idx = fraud_indices[0]\n",
        "    example_idx_in_test = list(y_test.index).index(example_idx)\n",
        "    \n",
        "    print(f\"\\nüìã –ü—Ä–∏–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ #{example_idx_in_test}:\")\n",
        "    print(f\"   –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è (1)\")\n",
        "    print(f\"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {y_pred_proba_selected[example_idx_in_test]:.4f}\")\n",
        "    print(f\"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {'–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è' if y_pred_optimal[example_idx_in_test] == 1 else '–ù–æ—Ä–º–∞–ª—å–Ω–∞—è'}\")\n",
        "    \n",
        "    # Waterfall plot –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.waterfall_plot(\n",
        "        shap.Explanation(\n",
        "            values=shap_values_fraud[example_idx_in_test],\n",
        "            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
        "            data=X_test_selected.iloc[example_idx_in_test].values,\n",
        "            feature_names=selected_features\n",
        "        ),\n",
        "        max_display=15,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f'SHAP Waterfall Plot - Transaction #{example_idx_in_test}\\n(Predicted: {y_pred_proba_selected[example_idx_in_test]:.4f})', \n",
        "              fontsize=12, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../model/shap_waterfall_example.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\")\n",
        "    \n",
        "    # –ë–µ—Ä–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\n",
        "    high_prob_idx = np.argmax(y_pred_proba_selected)\n",
        "    print(f\"\\nüìã –ü—Ä–∏–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞:\")\n",
        "    print(f\"   –ò–Ω–¥–µ–∫—Å: {high_prob_idx}\")\n",
        "    print(f\"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {y_pred_proba_selected[high_prob_idx]:.4f}\")\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.waterfall_plot(\n",
        "        shap.Explanation(\n",
        "            values=shap_values_fraud[high_prob_idx],\n",
        "            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,\n",
        "            data=X_test_selected.iloc[high_prob_idx].values,\n",
        "            feature_names=selected_features\n",
        "        ),\n",
        "        max_display=15,\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f'SHAP Waterfall Plot - High Risk Transaction\\n(Predicted: {y_pred_proba_selected[high_prob_idx]:.4f})', \n",
        "              fontsize=12, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../model/shap_waterfall_example.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 56: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å SHAP –≥—Ä–∞—Ñ–∏–∫–∏ (—É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—ã—à–µ)\n",
        "print(\"‚úÖ –í—Å–µ SHAP –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
        "print(\"  - shap_summary_plot.png\")\n",
        "print(\"  - shap_bar_plot.png\")\n",
        "print(\"  - shap_waterfall_example.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 57: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ model.pkl\n",
        "import os\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é model –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
        "os.makedirs('../model', exist_ok=True)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å, —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø–æ—Ä–æ–≥\n",
        "model_package = {\n",
        "    'model': final_model,\n",
        "    'selected_features': selected_features,\n",
        "    'threshold': final_threshold,\n",
        "    'scaler': None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è LightGBM\n",
        "    'model_type': 'LightGBM',\n",
        "    'metrics': final_metrics\n",
        "}\n",
        "\n",
        "joblib.dump(model_package, '../model/model.pkl')\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ../model/model.pkl\")\n",
        "\n",
        "# –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
        "joblib.dump(final_model, '../model/final_model.pkl')\n",
        "joblib.dump(selected_features, '../model/selected_features.pkl')\n",
        "joblib.dump(final_threshold, '../model/threshold.pkl')\n",
        "\n",
        "print(\"‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\")\n",
        "print(\"  - final_model.pkl\")\n",
        "print(\"  - selected_features.pkl\")\n",
        "print(\"  - threshold.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
