{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç:\n",
        "- **–ó–∞–¥–∞—á–∞ 29**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "- **–ó–∞–¥–∞—á–∞ 30**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "- **–ó–∞–¥–∞—á–∞ 31**: –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤ 01_full_data_pipeline.ipynb)\n",
        "- **–ó–∞–¥–∞—á–∞ 32**: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ features.csv (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤ 01_full_data_pipeline.ipynb)\n",
        "- **–ó–∞–¥–∞—á–∞ 34**: Preprocessing pipeline\n",
        "- **–ó–∞–¥–∞—á–∞ 36**: –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING: –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "FEATURES_FILE = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FEATURES_FILE)\n",
        "    print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}\")\n",
        "    print(f\"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}\")\n",
        "    print(f\"–°—Ç—Ä–æ–∫: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå –§–∞–π–ª {FEATURES_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    print(\"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ 01_full_data_pipeline.ipynb –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 29: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\\n\")\n",
        "\n",
        "# –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–µ —á–∏—Å–ª–æ–≤—ã—Ö –∏ –Ω–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö)\n",
        "categorical_cols = []\n",
        "for col in df.columns:\n",
        "    if col in ['target', 'user_id']:\n",
        "        continue\n",
        "    if df[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    elif df[col].dtype in [np.int64, np.int32] and df[col].nunique() < 20:\n",
        "        # –¶–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ç–æ–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏\n",
        "        if col not in ['hour', 'day_of_week', 'day_of_month', 'month']:\n",
        "            categorical_cols.append(col)\n",
        "\n",
        "print(f\"–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_cols)}\")\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:\")\n",
        "    for i, col in enumerate(categorical_cols[:10], 1):\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"  {i}. {col} ({unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)\")\n",
        "    if len(categorical_cols) > 10:\n",
        "        print(f\"  ... –∏ –µ—â–µ {len(categorical_cols) - 10} –∫–æ–ª–æ–Ω–æ–∫\")\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "processed_categorical = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    df[col].fillna('UNKNOWN', inplace=True)\n",
        "    \n",
        "    unique_count = df[col].nunique()\n",
        "    \n",
        "    # Frequency Encoding (–¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö)\n",
        "    freq_counts = df[col].value_counts()\n",
        "    df[f'{col}_freq'] = df[col].map(freq_counts) / len(df)\n",
        "    processed_categorical.append(f'{col}_freq')\n",
        "    \n",
        "    # Target Encoding (–µ—Å–ª–∏ –µ—Å—Ç—å target) - —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ target –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "    if 'target' in df.columns:\n",
        "        target_mean = df.groupby(col)['target'].mean()\n",
        "        df[f'{col}_target_enc'] = df[col].map(target_mean)\n",
        "        df[f'{col}_target_enc'].fillna(df['target'].mean(), inplace=True)  # –î–ª—è –Ω–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "        processed_categorical.append(f'{col}_target_enc')\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É (–ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è encoding)\n",
        "    if unique_count > 10:  # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        df.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω {col}: —Å–æ–∑–¥–∞–Ω frequency encoding, —É–¥–∞–ª–µ–Ω–∞ –∏—Å—Ö–æ–¥–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞\")\n",
        "    else:\n",
        "        print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω {col}: —Å–æ–∑–¥–∞–Ω frequency encoding, –∏—Å—Ö–æ–¥–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\")\n",
        "\n",
        "print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(categorical_cols)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(processed_categorical)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 30: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ ---\\n\")\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in ['target', 'user_id']]\n",
        "\n",
        "print(f\"–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "\n",
        "# –ú–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "outliers_info = []\n",
        "\n",
        "for col in numeric_cols[:20]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å)\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    # –ú–µ—Ç–æ–¥ IQR (Interquartile Range)\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers_iqr = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "    outliers_pct = (outliers_iqr / len(df)) * 100\n",
        "    \n",
        "    # –ú–µ—Ç–æ–¥ Z-score (–≤—ã–±—Ä–æ—Å—ã > 3 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)\n",
        "    z_scores = np.abs((df[col] - df[col].mean()) / (df[col].std() + 1e-8))\n",
        "    outliers_zscore = (z_scores > 3).sum()\n",
        "    outliers_zscore_pct = (outliers_zscore / len(df)) * 100\n",
        "    \n",
        "    if outliers_iqr > 0 or outliers_zscore > 0:\n",
        "        outliers_info.append({\n",
        "            'column': col,\n",
        "            'outliers_iqr': outliers_iqr,\n",
        "            'outliers_iqr_pct': outliers_pct,\n",
        "            'outliers_zscore': outliers_zscore,\n",
        "            'outliers_zscore_pct': outliers_zscore_pct\n",
        "        })\n",
        "\n",
        "if len(outliers_info) > 0:\n",
        "    outliers_df = pd.DataFrame(outliers_info)\n",
        "    outliers_df = outliers_df.sort_values('outliers_iqr', ascending=False)\n",
        "    \n",
        "    print(\"–¢–û–ü-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(outliers_df.head(10).to_string(index=False))\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    top_5_cols = outliers_df.head(5)['column'].tolist()\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, col in enumerate(top_5_cols[:6]):\n",
        "        if col in df.columns:\n",
        "            # Box plot –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "            axes[i].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
        "                          boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "            axes[i].set_title(f'{col[:40]}...\\n–í—ã–±—Ä–æ—Å–æ–≤: {outliers_df[outliers_df[\"column\"]==col][\"outliers_iqr\"].values[0]:.0f}', \n",
        "                           fontsize=10, fontweight='bold')\n",
        "            axes[i].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')\n",
        "            axes[i].grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    # –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—à–Ω–∏–µ subplot'—ã\n",
        "    for i in range(len(top_5_cols), 6):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.suptitle('–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ (—Ç–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—ã–±—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞!\")\n",
        "    print(\"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ù–ï —É–¥–∞–ª—è—Ç—å –≤—ã–±—Ä–æ—Å—ã, –∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, —É—Å—Ç–æ–π—á–∏–≤—ã–µ –∫ –Ω–∏–º\")\n",
        "    print(\"   (–Ω–∞–ø—Ä–∏–º–µ—Ä, Random Forest, XGBoost, LightGBM)\")\n",
        "else:\n",
        "    print(\"‚úÖ –í—ã–±—Ä–æ—Å–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (–∏–ª–∏ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã)\")\n",
        "\n",
        "print(\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 31: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\\n\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1] –∏–ª–∏ –æ–∫–æ–ª–æ —Ç–æ–≥–æ)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in ['target', 'user_id']]\n",
        "\n",
        "# –ò—Å–∫–ª—é—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –∏ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "cols_to_check = [col for col in numeric_cols \n",
        "                 if not any(x in col.lower() for x in ['_sin', '_cos', 'is_', 'changed', 'hour', 'day_of_week', 'day_of_month', 'month'])]\n",
        "\n",
        "print(f\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {len(cols_to_check)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "scaling_status = []\n",
        "for col in cols_to_check[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 15\n",
        "    if col in df.columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        mean_val = df[col].mean()\n",
        "        \n",
        "        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1] –∏–ª–∏ –±–ª–∏–∑–∫–æ –∫ –Ω–µ–º—É - –≤–µ—Ä–æ—è—Ç–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ\n",
        "        is_scaled = (min_val >= -0.1 and max_val <= 1.1) or (abs(mean_val) < 1 and abs(min_val) < 10 and abs(max_val) < 10)\n",
        "        \n",
        "        scaling_status.append({\n",
        "            'column': col,\n",
        "            'min': min_val,\n",
        "            'max': max_val,\n",
        "            'mean': mean_val,\n",
        "            'likely_scaled': is_scaled\n",
        "        })\n",
        "\n",
        "if len(scaling_status) > 0:\n",
        "    scaling_df = pd.DataFrame(scaling_status)\n",
        "    print(\"–°—Ç–∞—Ç—É—Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(scaling_df.to_string(index=False))\n",
        "    \n",
        "    scaled_count = scaling_df['likely_scaled'].sum()\n",
        "    print(f\"\\n‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ: {scaled_count} –∏–∑ {len(scaling_df)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\")\n",
        "\n",
        "print(\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–ê–î–ê–ß–ê 32: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å features.csv\n",
        "\n",
        "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –≠—Ç–∞ –∑–∞–¥–∞—á–∞ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤ `01_full_data_pipeline.ipynb`.\n",
        "–ó–¥–µ—Å—å –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 32: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ features.csv ---\\n\")\n",
        "\n",
        "FEATURES_OUTPUT = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "if os.path.exists(FEATURES_OUTPUT):\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞\n",
        "    file_size = os.path.getsize(FEATURES_OUTPUT) / (1024 * 1024)  # –≤ MB\n",
        "    \n",
        "    print(f\"‚úÖ –§–∞–π–ª features.csv —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {FEATURES_OUTPUT}\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} MB\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}\")\n",
        "    print(f\"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target): {len([c for c in df.columns if c != 'target'])}\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ target\n",
        "    if 'target' in df.columns:\n",
        "        target_dist = df['target'].value_counts()\n",
        "        print(f\"\\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\")\n",
        "        for val, count in target_dist.items():\n",
        "            print(f\"     –ö–ª–∞—Å—Å {val}: {count} ({count/len(df)*100:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ –§–∞–π–ª features.csv –≥–æ—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ ML –∏–Ω–∂–∏–Ω–µ—Ä—É!\")\n",
        "else:\n",
        "    print(f\"‚ùå –§–∞–π–ª {FEATURES_OUTPUT} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    print(\"   –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ–π—á–∞—Å...\")\n",
        "    \n",
        "    output_dir = os.path.dirname(FEATURES_OUTPUT)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    df.to_csv(FEATURES_OUTPUT, index=False)\n",
        "    print(f\"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {FEATURES_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–ê–î–ê–ß–ê 34: –ù–∞–ø–∏—Å–∞—Ç—å preprocessing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 34: –°–æ–∑–¥–∞–Ω–∏–µ preprocessing pipeline ---\\n\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è preprocessing pipeline\n",
        "# –≠—Ç–æ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è ML –∏–Ω–∂–∏–Ω–µ—Ä–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "def create_preprocessing_pipeline(df_train, df_test=None):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç preprocessing pipeline –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
        "    \n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - df_train: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    - df_test: —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    \n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - X_train, y_train: –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "    - X_test, y_test: –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ç–µ—Å—Ç–∞ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)\n",
        "    \"\"\"\n",
        "    \n",
        "    # –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    train = df_train.copy()\n",
        "    \n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "    if 'target' in train.columns:\n",
        "        y_train = train['target'].copy()\n",
        "        X_train = train.drop(columns=['target'], errors='ignore')\n",
        "    else:\n",
        "        y_train = None\n",
        "        X_train = train.copy()\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    cols_to_drop = ['user_id', 'timestamp'] if 'user_id' in X_train.columns else []\n",
        "    X_train = X_train.drop(columns=cols_to_drop, errors='ignore')\n",
        "    \n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    X_train = X_train.fillna(0)\n",
        "    \n",
        "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)\n",
        "    if df_test is not None:\n",
        "        test = df_test.copy()\n",
        "        \n",
        "        if 'target' in test.columns:\n",
        "            y_test = test['target'].copy()\n",
        "            X_test = test.drop(columns=['target'], errors='ignore')\n",
        "        else:\n",
        "            y_test = None\n",
        "            X_test = test.copy()\n",
        "        \n",
        "        X_test = X_test.drop(columns=cols_to_drop, errors='ignore')\n",
        "        X_test = X_test.fillna(0)\n",
        "        \n",
        "        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç\n",
        "        missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "        if missing_cols:\n",
        "            for col in missing_cols:\n",
        "                X_test[col] = 0\n",
        "        \n",
        "        X_test = X_test[X_train.columns]  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ç–æ–º—É –∂–µ –ø–æ—Ä—è–¥–∫—É\n",
        "        \n",
        "        return X_train, y_train, X_test, y_test\n",
        "    \n",
        "    return X_train, y_train\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è pipeline\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è create_preprocessing_pipeline()\")\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ preprocessing pipeline\n",
        "X_train, y_train, X_test, y_test = create_preprocessing_pipeline(train_df, test_df)\n",
        "\n",
        "# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å:\n",
        "# from lightgbm import LGBMClassifier\n",
        "# model = LGBMClassifier()\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "\"\"\")\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º pipeline –Ω–∞ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "if 'target' in df.columns:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = create_preprocessing_pipeline(train_df, test_df)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Pipeline –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω:\")\n",
        "    print(f\"   X_train shape: {X_train.shape}\")\n",
        "    print(f\"   X_test shape: {X_test.shape}\")\n",
        "    print(f\"   y_train distribution: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"   y_test distribution: {y_test.value_counts().to_dict()}\")\n",
        "    print(f\"\\n‚úÖ Preprocessing pipeline –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, pipeline –Ω–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
        "\n",
        "print(\"\\n‚úÖ –ó–∞–¥–∞—á–∞ 34 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞: preprocessing pipeline —Å–æ–∑–¥–∞–Ω\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 36: –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\\n\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target –∏ user_id)\n",
        "feature_cols = [col for col in df.columns if col not in ['target', 'user_id']]\n",
        "\n",
        "print(f\"üìä –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í: {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "temporal_features = [col for col in feature_cols if any(x in col for x in ['hour', 'day', 'month', 'weekend', 'night', '_sin', '_cos'])]\n",
        "user_features = [col for col in feature_cols if 'user_' in col or 'amount_diff' in col or 'amount_ratio' in col]\n",
        "rolling_features = [col for col in feature_cols if any(x in col for x in ['tx_count', 'tx_mean', 'tx_std', 'tx_sum', 'tx_max'])]\n",
        "statistical_features = [col for col in feature_cols if any(x in col for x in ['zscore', 'percentile', 'ratio', 'cv_amount'])]\n",
        "device_features = [col for col in feature_cols if any(x in col for x in ['device', 'os', 'ip', 'geo'])]\n",
        "behavioral_features = [col for col in feature_cols if any(x in col for x in ['time_since', 'time_until', 'rate', 'interval', 'rapid', 'anomaly', 'morning', 'afternoon', 'evening'])]\n",
        "categorical_features = [col for col in feature_cols if '_freq' in col or '_target_enc' in col]\n",
        "other_features = [col for col in feature_cols if col not in temporal_features + user_features + rolling_features + statistical_features + device_features + behavioral_features + categorical_features]\n",
        "\n",
        "categories = {\n",
        "    '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏': temporal_features,\n",
        "    '–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é': user_features,\n",
        "    'Rolling-window –ø—Ä–∏–∑–Ω–∞–∫–∏': rolling_features,\n",
        "    '–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏': statistical_features,\n",
        "    'Device/OS/IP –ø—Ä–∏–∑–Ω–∞–∫–∏': device_features,\n",
        "    '–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è': behavioral_features,\n",
        "    '–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ encoding': categorical_features,\n",
        "    '–î—Ä—É–≥–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏': other_features\n",
        "}\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "print(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\\n\")\n",
        "for category, cols in categories.items():\n",
        "    if len(cols) > 0:\n",
        "        print(f\"  {category}: {len(cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "print(f\"\\n  –í–°–ï–ì–û: {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ñ–∞–π–ª\n",
        "features_list_file = '../data/processed/features_list.txt'\n",
        "with open(features_list_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"–§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø ML –ú–û–î–ï–õ–ò\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    f.write(f\"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}\\n\\n\")\n",
        "    \n",
        "    for category, cols in categories.items():\n",
        "        if len(cols) > 0:\n",
        "            f.write(f\"{category} ({len(cols)}):\\n\")\n",
        "            for col in cols:\n",
        "                f.write(f\"  - {col}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {features_list_file}\")\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–µ—Ä–≤—ã–µ 20):\")\n",
        "for i, col in enumerate(feature_cols[:20], 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "if len(feature_cols) > 20:\n",
        "    print(f\"  ... –∏ –µ—â–µ {len(feature_cols) - 20} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "print(f\"\\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–∑ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≥–æ—Ç–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–∏!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n‚úÖ –í–´–ü–û–õ–ù–ï–ù–ù–´–ï –ó–ê–î–ê–ß–ò:\")\n",
        "print(\"  29. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  30. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  31. –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ - –í–´–ü–û–õ–ù–ï–ù–û (–≤ 01_full_data_pipeline.ipynb)\")\n",
        "print(\"  32. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ features.csv - –í–´–ü–û–õ–ù–ï–ù–û (–≤ 01_full_data_pipeline.ipynb)\")\n",
        "print(\"  33. –°–æ–∑–¥–∞–Ω–∏–µ notebook 02_feature_engineering.ipynb - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  34. Preprocessing pipeline - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  36. –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "\n",
        "print(\"\\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
        "print(f\"  - –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len([c for c in df.columns if c != 'target'])}\")\n",
        "print(f\"  - –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
        "if 'target' in df.columns:\n",
        "    target_dist = df['target'].value_counts()\n",
        "    print(f\"  - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {target_dist.to_dict()}\")\n",
        "\n",
        "print(\"\\nüìÅ –§–ê–ô–õ–´ –î–õ–Ø ML –ò–ù–ñ–ò–ù–ï–†–ê:\")\n",
        "print(\"  - data/processed/transactions_with_features.csv - —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\")\n",
        "print(\"  - data/processed/features_list.txt - —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(\"  - FEATURES_DOCUMENTATION.md - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º\")\n",
        "print(\"  - notebooks/02_feature_engineering.ipynb - preprocessing pipeline\")\n",
        "\n",
        "print(\"\\n‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò FEATURE ENGINEERING –í–´–ü–û–õ–ù–ï–ù–´!\")\n",
        "print(\"   –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ ML –∏–Ω–∂–∏–Ω–µ—Ä—É. üöÄ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
