{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка визуал\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\" Все библиотеки загружены успешно!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "INPUT_PATH = '../data/processed/transactions_with_features.csv'\n",
        "OUTPUT_PATH = '../data/processed/transactions_with_features_final.csv'\n",
        "PIPELINE_PATH = '../model/preprocessing_pipeline.pkl'\n",
        "\n",
        "\n",
        "df = pd.read_csv(INPUT_PATH, low_memory=False)\n",
        "print(f\" Данные загружены: {df.shape}\")\n",
        "print(f\"Колонок: {len(df.columns)}\")\n",
        "print(f\"Строк: {len(df)}\")\n",
        "\n",
        "# Проверка целевой переменной\n",
        "if 'target' in df.columns:\n",
        "    print(f\"\\n Распределение классов:\")\n",
        "    print(df['target'].value_counts())\n",
        "    print(f\"Дисбаланс: {df['target'].value_counts()[0] / df['target'].value_counts()[1]:.2f}:1\")\n",
        "else:\n",
        "    raise ValueError(\" Колонка 'target' не найдена!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Находим категориальные признаки\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"\\nНайдено категориальных колонок: {len(categorical_cols)}\")\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"Категориальные колонки: {categorical_cols}\")\n",
        "\n",
        "# Обработка категориальных признаков\n",
        "label_encoders = {}\n",
        "frequency_encodings = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col == 'target' or col == 'user_id':\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n--- Обработка колонки: {col} ---\")\n",
        "    \n",
        "    # Проверяем количество уникальных значений\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"  Уникальных значений: {unique_count}\")\n",
        "    \n",
        "    # Если много уникальных значений (>50) - используем frequency encoding\n",
        "    # Если мало - можно использовать label encoding или one-hot\n",
        "    if unique_count > 50:\n",
        "        # Frequency Encoding для колонок с большим количеством уникальных значений\n",
        "        freq_map = df[col].value_counts() / len(df)\n",
        "        df[f'{col}_freq_encoding'] = df[col].map(freq_map)\n",
        "        frequency_encodings[col] = freq_map\n",
        "        print(f\"   Создан frequency encoding: {col}_freq_encoding\")\n",
        "        # Удаляем исходную колонку\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "    elif unique_count > 2:\n",
        "        # Label Encoding для колонок с небольшим количеством уникальных значений\n",
        "        le = LabelEncoder()\n",
        "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str).fillna('UNKNOWN'))\n",
        "        label_encoders[col] = le\n",
        "        print(f\"   Создан label encoding: {col}_encoded\")\n",
        "        # Удаляем исходную колонку\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "    else:\n",
        "        # Бинарные признаки - можно оставить как есть или закодировать\n",
        "        df[col] = df[col].astype(str).fillna('UNKNOWN')\n",
        "        print(f\"   Бинарный признак оставлен как есть\")\n",
        "\n",
        "print(f\"\\n Обработка категориальных признаков завершена!\")\n",
        "print(f\"Создано label encoders: {len(label_encoders)}\")\n",
        "print(f\"Создано frequency encodings: {len(frequency_encodings)}\")\n",
        "print(f\"Текущий размер данных: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Выбираем только числовые колонки\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'target' in numeric_cols:\n",
        "    numeric_cols.remove('target')\n",
        "if 'user_id' in numeric_cols:\n",
        "    numeric_cols.remove('user_id')\n",
        "\n",
        "print(f\"\\nПроверяем {len(numeric_cols)} числовых признаков на выбросы...\")\n",
        "\n",
        "# Метод IQR для обнаружения выбросов\n",
        "outliers_info = {}\n",
        "outliers_count = 0\n",
        "\n",
        "for col in numeric_cols[:20]:  # Проверяем первые 20 для скорости\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    outlier_count = len(outliers)\n",
        "    \n",
        "    if outlier_count > 0:\n",
        "        outliers_info[col] = {\n",
        "            'count': outlier_count,\n",
        "            'percentage': (outlier_count / len(df)) * 100,\n",
        "            'lower_bound': lower_bound,\n",
        "            'upper_bound': upper_bound\n",
        "        }\n",
        "        outliers_count += outlier_count\n",
        "\n",
        "print(f\"\\n Обнаружено выбросов: {outliers_count}\")\n",
        "print(f\"Колонок с выбросами: {len(outliers_info)}\")\n",
        "\n",
        "# Показываем топ-10 колонок с наибольшим количеством выбросов\n",
        "if len(outliers_info) > 0:\n",
        "    sorted_outliers = sorted(outliers_info.items(), key=lambda x: x[1]['count'], reverse=True)[:10]\n",
        "    print(\"\\nТоп-10 колонок с выбросами:\")\n",
        "    for col, info in sorted_outliers:\n",
        "        print(f\"  {col}: {info['count']} выбросов ({info['percentage']:.2f}%)\")\n",
        "\n",
        "# Решение: Для fraud detection выбросы важны (аномалии = мошенничество)\n",
        "# Поэтому логируем их\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Выбираем числовые колонки для масштабирования\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Исключаем колонки, которые НЕ нужно масштабировать:\n",
        "# - target (целевая переменная)\n",
        "# - user_id (идентификатор)\n",
        "# - бинарные признаки (0/1)\n",
        "# - циклические признаки (sin/cos уже в диапазоне -1 до 1)\n",
        "exclude_cols = ['target', 'user_id']\n",
        "exclude_patterns = ['_sin', '_cos', 'is_', '_encoded']  # Циклические и бинарные\n",
        "\n",
        "cols_to_scale = []\n",
        "for col in numeric_cols:\n",
        "    if col in exclude_cols:\n",
        "        continue\n",
        "    if any(pattern in col for pattern in exclude_patterns):\n",
        "        continue\n",
        "    # Проверяем, что это не бинарный признак (только 0 и 1)\n",
        "    unique_vals = df[col].dropna().unique()\n",
        "    if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
        "        continue\n",
        "    cols_to_scale.append(col)\n",
        "\n",
        "print(f\"\\nНайдено {len(cols_to_scale)} признаков для масштабирования\")\n",
        "print(f\"Примеры: {cols_to_scale[:5]}\")\n",
        "\n",
        "# Используем MinMaxScaler для масштабирования в диапазон [0, 1]\n",
        "# Это лучше для tree-based моделей (LightGBM, XGBoost)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Сохраняем исходные данные для масштабирования\n",
        "df_scaled = df.copy()\n",
        "df_scaled[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "\n",
        "# Обновляем основной датафрейм\n",
        "df = df_scaled\n",
        "\n",
        "print(f\"\\n Масштабировано {len(cols_to_scale)} признаков с помощью MinMaxScaler\")\n",
        "print(\"Признаки масштабированы в диапазон [0, 1]\")\n",
        "\n",
        "# Сохраняем scaler для использования в pipeline\n",
        "print(f\"\\nScaler сохранен для использования в preprocessing pipeline\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ЗАДАЧА 32: СОХРАНЕНИЕ ФИНАЛЬНОГО ДАТАСЕТА\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Заполняем оставшиеся пропуски\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Убеждаемся, что все колонки числовые (кроме user_id если он нужен)\n",
        "# Удаляем user_id если он не нужен для модели\n",
        "if 'user_id' in df.columns:\n",
        "    # Сохраняем user_id отдельно для возможного использования\n",
        "    user_ids = df['user_id'].copy()\n",
        "    df_for_model = df.drop(columns=['user_id'], errors='ignore')\n",
        "else:\n",
        "    df_for_model = df.copy()\n",
        "\n",
        "# Проверяем финальную структуру\n",
        "print(f\"\\n Финальная структура данных:\")\n",
        "print(f\"  Размер: {df_for_model.shape}\")\n",
        "print(f\"  Колонок: {len(df_for_model.columns)}\")\n",
        "print(f\"  Строк: {len(df_for_model)}\")\n",
        "\n",
        "# Проверяем наличие target\n",
        "if 'target' not in df_for_model.columns:\n",
        "    raise ValueError(\" Колонка 'target' отсутствует!\")\n",
        "\n",
        "# Сохраняем финальный датасет\n",
        "output_dir = os.path.dirname(OUTPUT_PATH)\n",
        "if output_dir and not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "df_for_model.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"\\n Финальный датасет сохранен: {OUTPUT_PATH}\")\n",
        "\n",
        "# Также сохраняем версию с user_id (если нужна)\n",
        "if 'user_id' in df.columns:\n",
        "    df.to_csv(OUTPUT_PATH.replace('_final.csv', '_with_user_id.csv'), index=False)\n",
        "    print(f\" Версия с user_id сохранена: {OUTPUT_PATH.replace('_final.csv', '_with_user_id.csv')}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Создаем класс для preprocessing pipeline\n",
        "class FraudDetectionPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline для fraud detection модели.\n",
        "    Выполняет все необходимые преобразования данных.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, label_encoders=None, frequency_encodings=None, scaler=None, feature_columns=None):\n",
        "        self.label_encoders = label_encoders or {}\n",
        "        self.frequency_encodings = frequency_encodings or {}\n",
        "        self.scaler = scaler\n",
        "        self.feature_columns = feature_columns  # Список финальных признаков\n",
        "        \n",
        "    def fit(self, df):\n",
        "        \"\"\"Обучение pipeline на данных\"\"\"\n",
        "        # Сохраняем список финальных признаков (без target и user_id)\n",
        "        if self.feature_columns is None:\n",
        "            self.feature_columns = [col for col in df.columns \n",
        "                                   if col not in ['target', 'user_id']]\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df):\n",
        "        \"\"\"Применение преобразований к данным\"\"\"\n",
        "        df_processed = df.copy()\n",
        "        \n",
        "        # 1. Обработка категориальных признаков\n",
        "        for col, le in self.label_encoders.items():\n",
        "            if col in df_processed.columns:\n",
        "                df_processed[f'{col}_encoded'] = le.transform(\n",
        "                    df_processed[col].astype(str).fillna('UNKNOWN')\n",
        "                )\n",
        "                df_processed.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        \n",
        "        for col, freq_map in self.frequency_encodings.items():\n",
        "            if col in df_processed.columns:\n",
        "                df_processed[f'{col}_freq_encoding'] = df_processed[col].map(freq_map).fillna(0)\n",
        "                df_processed.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        \n",
        "        # 2. Заполнение пропусков\n",
        "        df_processed.fillna(0, inplace=True)\n",
        "        \n",
        "        # 3. Масштабирование (если scaler предоставлен)\n",
        "        if self.scaler is not None and self.feature_columns is not None:\n",
        "            available_cols = [col for col in self.feature_columns if col in df_processed.columns]\n",
        "            if available_cols:\n",
        "                df_processed[available_cols] = self.scaler.transform(df_processed[available_cols])\n",
        "        \n",
        "        # 4. Выбор только нужных признаков\n",
        "        if self.feature_columns is not None:\n",
        "            # Сохраняем target и user_id если они есть\n",
        "            keep_cols = self.feature_columns.copy()\n",
        "            if 'target' in df_processed.columns:\n",
        "                keep_cols.append('target')\n",
        "            if 'user_id' in df_processed.columns:\n",
        "                keep_cols.append('user_id')\n",
        "            df_processed = df_processed[[col for col in keep_cols if col in df_processed.columns]]\n",
        "        \n",
        "        return df_processed\n",
        "    \n",
        "    def fit_transform(self, df):\n",
        "        \"\"\"Обучение и применение pipeline\"\"\"\n",
        "        return self.fit(df).transform(df)\n",
        "\n",
        "# Создаем и сохраняем pipeline\n",
        "preprocessor = FraudDetectionPreprocessor(\n",
        "    label_encoders=label_encoders,\n",
        "    frequency_encodings=frequency_encodings,\n",
        "    scaler=scaler,\n",
        "    feature_columns=[col for col in df_for_model.columns if col != 'target']\n",
        ")\n",
        "\n",
        "# Сохраняем pipeline\n",
        "os.makedirs(os.path.dirname(PIPELINE_PATH), exist_ok=True)\n",
        "joblib.dump(preprocessor, PIPELINE_PATH)\n",
        "print(f\"\\n Preprocessing pipeline сохранен: {PIPELINE_PATH}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def validate_input_data(df, required_features=None, target_col='target'):\n",
        "    \"\"\"\n",
        "    Валидация входных данных для fraud detection модели.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Входные данные для валидации\n",
        "    required_features : list, optional\n",
        "        Список обязательных признаков. Если None, берется из сохраненной модели\n",
        "    target_col : str, default='target'\n",
        "        Название колонки с целевой переменной (если есть)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Словарь с результатами валидации\n",
        "        - 'is_valid': bool - прошла ли валидация\n",
        "        - 'errors': list - список ошибок\n",
        "        - 'warnings': list - список предупреждений\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    warnings = []\n",
        "    \n",
        "    # 1. Проверка типа данных\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        errors.append(\"Входные данные должны быть pandas.DataFrame\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    # 2. Проверка наличия данных\n",
        "    if df.empty:\n",
        "        errors.append(\"DataFrame пуст\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    # 3. Проверка обязательных признаков\n",
        "    if required_features is not None:\n",
        "        missing_features = [f for f in required_features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            errors.append(f\"Отсутствуют обязательные признаки: {missing_features[:10]}\")\n",
        "    \n",
        "    # 4. Проверка типов данных (все должны быть числовыми, кроме target и user_id)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    non_numeric_cols = [col for col in df.columns \n",
        "                       if col not in numeric_cols and col not in [target_col, 'user_id']]\n",
        "    if non_numeric_cols:\n",
        "        errors.append(f\"Найдены нечисловые колонки (кроме target/user_id): {non_numeric_cols[:5]}\")\n",
        "    \n",
        "    # 5. Проверка пропущенных значений\n",
        "    missing_counts = df.isnull().sum()\n",
        "    cols_with_missing = missing_counts[missing_counts > 0]\n",
        "    if len(cols_with_missing) > 0:\n",
        "        warnings.append(f\"Найдены пропущенные значения в {len(cols_with_missing)} колонках\")\n",
        "        warnings.append(f\"Примеры: {dict(cols_with_missing.head(5))}\")\n",
        "    \n",
        "    # 6. Проверка бесконечных значений\n",
        "    inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()\n",
        "    cols_with_inf = inf_counts[inf_counts > 0]\n",
        "    if len(cols_with_inf) > 0:\n",
        "        errors.append(f\"Найдены бесконечные значения в {len(cols_with_inf)} колонках\")\n",
        "    \n",
        "    # 7. Проверка размера данных\n",
        "    if len(df) == 0:\n",
        "        errors.append(\"Нет данных для обработки\")\n",
        "    elif len(df) > 1000000:\n",
        "        warnings.append(f\"Большой объем данных: {len(df)} строк. Обработка может занять время\")\n",
        "    \n",
        "    is_valid = len(errors) == 0\n",
        "    \n",
        "    return {\n",
        "        'is_valid': is_valid,\n",
        "        'errors': errors,\n",
        "        'warnings': warnings,\n",
        "        'shape': df.shape,\n",
        "        'numeric_cols_count': len(numeric_cols),\n",
        "        'missing_values_count': missing_counts.sum()\n",
        "    }\n",
        "\n",
        "# Тестируем функцию валидации на наших данных\n",
        "print(\"\\n--- Тестирование функции валидации ---\")\n",
        "validation_result = validate_input_data(df_for_model, \n",
        "                                        required_features=[col for col in df_for_model.columns if col != 'target'])\n",
        "\n",
        "print(f\"\\nРезультат валидации:\")\n",
        "print(f\"  Валидно: {validation_result['is_valid']}\")\n",
        "print(f\"  Ошибок: {len(validation_result['errors'])}\")\n",
        "print(f\"  Предупреждений: {len(validation_result['warnings'])}\")\n",
        "\n",
        "if validation_result['errors']:\n",
        "    print(f\"\\nОшибки:\")\n",
        "    for error in validation_result['errors']:\n",
        "        print(f\"   {error}\")\n",
        "\n",
        "if validation_result['warnings']:\n",
        "    print(f\"\\nПредупреждения:\")\n",
        "    for warning in validation_result['warnings']:\n",
        "        print(f\"   {warning}\")\n",
        "\n",
        "# Сохраняем функцию валидации в отдельный файл для использования в API\n",
        "validation_code = '''\n",
        "def validate_input_data(df, required_features=None, target_col='target'):\n",
        "    \"\"\"Валидация входных данных для fraud detection модели.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    \n",
        "    errors = []\n",
        "    warnings = []\n",
        "    \n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        errors.append(\"Входные данные должны быть pandas.DataFrame\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    if df.empty:\n",
        "        errors.append(\"DataFrame пуст\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    if required_features is not None:\n",
        "        missing_features = [f for f in required_features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            errors.append(f\"Отсутствуют обязательные признаки: {missing_features[:10]}\")\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    non_numeric_cols = [col for col in df.columns \n",
        "                       if col not in numeric_cols and col not in [target_col, 'user_id']]\n",
        "    if non_numeric_cols:\n",
        "        errors.append(f\"Найдены нечисловые колонки: {non_numeric_cols[:5]}\")\n",
        "    \n",
        "    missing_counts = df.isnull().sum()\n",
        "    cols_with_missing = missing_counts[missing_counts > 0]\n",
        "    if len(cols_with_missing) > 0:\n",
        "        warnings.append(f\"Найдены пропущенные значения в {len(cols_with_missing)} колонках\")\n",
        "    \n",
        "    inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()\n",
        "    cols_with_inf = inf_counts[inf_counts > 0]\n",
        "    if len(cols_with_inf) > 0:\n",
        "        errors.append(f\"Найдены бесконечные значения в {len(cols_with_inf)} колонках\")\n",
        "    \n",
        "    is_valid = len(errors) == 0\n",
        "    \n",
        "    return {\n",
        "        'is_valid': is_valid,\n",
        "        'errors': errors,\n",
        "        'warnings': warnings,\n",
        "        'shape': df.shape\n",
        "    }\n",
        "'''\n",
        "\n",
        "# Сохраняем функцию в файл\n",
        "validation_file_path = '../src/data_validation.py'\n",
        "os.makedirs(os.path.dirname(validation_file_path), exist_ok=True)\n",
        "with open(validation_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(validation_code)\n",
        "\n",
        "print(f\"\\n Функция валидации сохранена: {validation_file_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Получаем финальный список признаков (без target)\n",
        "final_features = [col for col in df_for_model.columns if col != 'target']\n",
        "\n",
        "print(f\"\\n ФИНАЛЬНЫЙ НАБОР ПРИЗНАКОВ:\")\n",
        "print(f\"  Всего признаков: {len(final_features)}\")\n",
        "print(f\"  Размер данных: {df_for_model.shape}\")\n",
        "\n",
        "# Группируем признаки по категориям\n",
        "feature_categories = {\n",
        "    'Временные': [f for f in final_features if any(x in f for x in ['hour', 'day', 'month', 'weekend', 'night', 'sin', 'cos'])],\n",
        "    'По пользователю': [f for f in final_features if 'user_' in f],\n",
        "    'Rolling window': [f for f in final_features if any(x in f for x in ['_1h', '_12h', '_24h'])],\n",
        "    'Статистические': [f for f in final_features if any(x in f for x in ['zscore', 'percentile', 'ratio', 'cv'])],\n",
        "    'Device/OS': [f for f in final_features if any(x in f for x in ['device', 'os'])],\n",
        "    'Поведенческие': [f for f in final_features if any(x in f for x in ['time_since', 'time_until', 'tx_rate', 'interval', 'rapid', 'anomaly'])],\n",
        "    'Поведенческие (исходные)': [f for f in final_features if 'количество' in f or 'среднее' in f or 'доля' in f or 'интервал' in f],\n",
        "    'Прочие': []\n",
        "}\n",
        "\n",
        "# Распределяем оставшиеся признаки\n",
        "categorized = set()\n",
        "for category, features in feature_categories.items():\n",
        "    categorized.update(features)\n",
        "\n",
        "feature_categories['Прочие'] = [f for f in final_features if f not in categorized]\n",
        "\n",
        "print(f\"\\n Распределение признаков по категориям:\")\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"  {category}: {len(features)} признаков\")\n",
        "        if len(features) <= 5:\n",
        "            print(f\"    Примеры: {features}\")\n",
        "        else:\n",
        "            print(f\"    Примеры: {features[:3]} ... и еще {len(features)-3}\")\n",
        "\n",
        "# Сохраняем список финальных признаков\n",
        "features_list_path = '../model/final_features_list.txt'\n",
        "with open(features_list_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"ФИНАЛЬНЫЙ НАБОР ПРИЗНАКОВ ДЛЯ ML МОДЕЛИ\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"Всего признаков: {len(final_features)}\\n\\n\")\n",
        "    for category, features in feature_categories.items():\n",
        "        if features:\n",
        "            f.write(f\"{category} ({len(features)} признаков):\\n\")\n",
        "            for feat in features:\n",
        "                f.write(f\"  - {feat}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"\\n Список признаков сохранен: {features_list_path}\")\n",
        "\n",
        "# Сохраняем также в pickle для удобства\n",
        "import joblib\n",
        "joblib.dump(final_features, '../model/final_features.pkl')\n",
        "print(f\" Список признаков сохранен (pickle): ../model/final_features.pkl\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Настройка визуализации\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Загрузка данных с признаками\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка финального датасета с признаками\n",
        "FEATURES_FILE = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FEATURES_FILE)\n",
        "    print(f\" Данные загружены: {df.shape}\")\n",
        "    print(f\"Колонок: {len(df.columns)}\")\n",
        "    print(f\"Строк: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\" Файл {FEATURES_FILE} не найден!\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "categorical_cols = []\n",
        "for col in df.columns:\n",
        "    if col in ['target', 'user_id']:\n",
        "        continue\n",
        "    if df[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    elif df[col].dtype in [np.int64, np.int32] and df[col].nunique() < 20:\n",
        "        # Целочисленные колонки с небольшим количеством уникальных значений тоже могут быть категориальными\n",
        "        if col not in ['hour', 'day_of_week', 'day_of_month', 'month']:\n",
        "            categorical_cols.append(col)\n",
        "\n",
        "print(f\"Найдено категориальных признаков: {len(categorical_cols)}\")\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"Категориальные колонки:\")\n",
        "    for i, col in enumerate(categorical_cols[:10], 1):\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"  {i}. {col} ({unique_count} уникальных значений)\")\n",
        "    if len(categorical_cols) > 10:\n",
        "        print(f\"  ... и еще {len(categorical_cols) - 10} колонок\")\n",
        "\n",
        "# Обработка категориальных признаков\n",
        "processed_categorical = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    # Заполняем пропуски\n",
        "    df[col].fillna('UNKNOWN', inplace=True)\n",
        "    \n",
        "    unique_count = df[col].nunique()\n",
        "    \n",
        "    # Frequency Encoding (для всех категориальных)\n",
        "    freq_counts = df[col].value_counts()\n",
        "    df[f'{col}_freq'] = df[col].map(freq_counts) / len(df)\n",
        "    processed_categorical.append(f'{col}_freq')\n",
        "    \n",
        "    # Target Encoding (если есть target) - среднее значение target для каждой категории\n",
        "    if 'target' in df.columns:\n",
        "        target_mean = df.groupby(col)['target'].mean()\n",
        "        df[f'{col}_target_enc'] = df[col].map(target_mean)\n",
        "        df[f'{col}_target_enc'].fillna(df['target'].mean(), inplace=True)  # Для новых категорий\n",
        "        processed_categorical.append(f'{col}_target_enc')\n",
        "    \n",
        "    # Удаляем исходную категориальную колонку (после создания encoding)\n",
        "    if unique_count > 10:  # Удаляем только если много уникальных значений\n",
        "        df.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        print(f\" Обработан {col}: создан frequency encoding, удалена исходная колонка\")\n",
        "    else:\n",
        "        print(f\" Обработан {col}: создан frequency encoding, исходная колонка сохранена\")\n",
        "\n",
        "print(f\"\\n Обработано {len(categorical_cols)} категориальных признаков\")\n",
        "print(f\"Создано {len(processed_categorical)} новых признаков из категориальных\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Выбираем только числовые колонки для анализа выбросов\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in ['target', 'user_id']]\n",
        "\n",
        "print(f\"Анализ выбросов для {len(numeric_cols)} числовых признаков\\n\")\n",
        "\n",
        "# Методы обнаружения выбросов\n",
        "outliers_info = []\n",
        "\n",
        "for col in numeric_cols[:20]:  # Анализируем первые 20 признаков (чтобы не перегружать)\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    # Метод IQR (Interquartile Range)\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers_iqr = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "    outliers_pct = (outliers_iqr / len(df)) * 100\n",
        "    \n",
        "    # Метод Z-score (выбросы > 3 стандартных отклонений)\n",
        "    z_scores = np.abs((df[col] - df[col].mean()) / (df[col].std() + 1e-8))\n",
        "    outliers_zscore = (z_scores > 3).sum()\n",
        "    outliers_zscore_pct = (outliers_zscore / len(df)) * 100\n",
        "    \n",
        "    if outliers_iqr > 0 or outliers_zscore > 0:\n",
        "        outliers_info.append({\n",
        "            'column': col,\n",
        "            'outliers_iqr': outliers_iqr,\n",
        "            'outliers_iqr_pct': outliers_pct,\n",
        "            'outliers_zscore': outliers_zscore,\n",
        "            'outliers_zscore_pct': outliers_zscore_pct\n",
        "        })\n",
        "\n",
        "if len(outliers_info) > 0:\n",
        "    outliers_df = pd.DataFrame(outliers_info)\n",
        "    outliers_df = outliers_df.sort_values('outliers_iqr', ascending=False)\n",
        "    \n",
        "    print(\"ТОП-10 признаков с наибольшим количеством выбросов (IQR метод):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(outliers_df.head(10).to_string(index=False))\n",
        "    \n",
        "    # Визуализация выбросов для топ-5 признаков\n",
        "    top_5_cols = outliers_df.head(5)['column'].tolist()\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, col in enumerate(top_5_cols[:6]):\n",
        "        if col in df.columns:\n",
        "            # Box plot для визуализации выбросов\n",
        "            axes[i].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
        "                          boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "            axes[i].set_title(f'{col[:40]}...\\nВыбросов: {outliers_df[outliers_df[\"column\"]==col][\"outliers_iqr\"].values[0]:.0f}', \n",
        "                           fontsize=10, fontweight='bold')\n",
        "            axes[i].set_ylabel('Значение')\n",
        "            axes[i].grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    # Скрываем лишние subplot'ы\n",
        "    for i in range(len(top_5_cols), 6):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.suptitle('Визуализация выбросов (топ-5 признаков)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n⚠️ ВНИМАНИЕ: Выбросы могут быть важными для обнаружения мошенничества!\")\n",
        "    print(\"   Рекомендуется НЕ удалять выбросы, а использовать модели, устойчивые к ним\")\n",
        "    print(\"   (например, Random Forest, XGBoost, LightGBM)\")\n",
        "else:\n",
        "    print(\" Выбросов не обнаружено (или все признаки уже обработаны)\")\n",
        "\n",
        "print(\"\\n Проверка выбросов завершена\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Проверяем, какие признаки уже масштабированы (должны быть в диапазоне [0, 1] или около того)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in ['target', 'user_id']]\n",
        "\n",
        "# Исключаем бинарные и циклические признаки\n",
        "cols_to_check = [col for col in numeric_cols \n",
        "                 if not any(x in col.lower() for x in ['_sin', '_cos', 'is_', 'changed', 'hour', 'day_of_week', 'day_of_month', 'month'])]\n",
        "\n",
        "print(f\"Проверка масштабирования для {len(cols_to_check)} признаков\\n\")\n",
        "\n",
        "# Проверяем диапазоны значений\n",
        "scaling_status = []\n",
        "for col in cols_to_check[:15]:  # Проверяем первые 15\n",
        "    if col in df.columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        mean_val = df[col].mean()\n",
        "        \n",
        "        # Если значения в диапазоне [0, 1] или близко к нему - вероятно масштабировано\n",
        "        is_scaled = (min_val >= -0.1 and max_val <= 1.1) or (abs(mean_val) < 1 and abs(min_val) < 10 and abs(max_val) < 10)\n",
        "        \n",
        "        scaling_status.append({\n",
        "            'column': col,\n",
        "            'min': min_val,\n",
        "            'max': max_val,\n",
        "            'mean': mean_val,\n",
        "            'likely_scaled': is_scaled\n",
        "        })\n",
        "\n",
        "if len(scaling_status) > 0:\n",
        "    scaling_df = pd.DataFrame(scaling_status)\n",
        "    print(\"Статус масштабирования признаков:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(scaling_df.to_string(index=False))\n",
        "    \n",
        "    scaled_count = scaling_df['likely_scaled'].sum()\n",
        "    print(f\"\\n Вероятно масштабировано: {scaled_count} из {len(scaling_df)} проверенных признаков\")\n",
        "else:\n",
        "    print(\" Не удалось проверить масштабирование\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "FEATURES_OUTPUT = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "if os.path.exists(FEATURES_OUTPUT):\n",
        "    # Проверяем размер файла\n",
        "    file_size = os.path.getsize(FEATURES_OUTPUT) / (1024 * 1024)  # в MB\n",
        "    \n",
        "    print(f\"   Файл features.csv существует: {FEATURES_OUTPUT}\")\n",
        "    print(f\"   Размер файла: {file_size:.2f} MB\")\n",
        "    print(f\"   Размер данных: {df.shape}\")\n",
        "    print(f\"   Количество признаков (без target): {len([c for c in df.columns if c != 'target'])}\")\n",
        "    \n",
        "    # Проверяем наличие target\n",
        "    if 'target' in df.columns:\n",
        "        target_dist = df['target'].value_counts()\n",
        "        print(f\"\\n   Распределение target:\")\n",
        "        for val, count in target_dist.items():\n",
        "            print(f\"     Класс {val}: {count} ({count/len(df)*100:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\n Файл features.csv готов!\")\n",
        "else:\n",
        "    print(f\" Файл {FEATURES_OUTPUT} не найден!\")\n",
        "    \n",
        "    \n",
        "    output_dir = os.path.dirname(FEATURES_OUTPUT)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    df.to_csv(FEATURES_OUTPUT, index=False)\n",
        "    print(f\" Файл сохранен: {FEATURES_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Создаем функцию для preprocessing pipeline\n",
        "\n",
        "def create_preprocessing_pipeline(df_train, df_test=None):\n",
        "\n",
        "    \n",
        "    # Копируем данные\n",
        "    train = df_train.copy()\n",
        "    \n",
        "    # Разделяем на признаки и целевую переменную\n",
        "    if 'target' in train.columns:\n",
        "        y_train = train['target'].copy()\n",
        "        X_train = train.drop(columns=['target'], errors='ignore')\n",
        "    else:\n",
        "        y_train = None\n",
        "        X_train = train.copy()\n",
        "    \n",
        "    # Удаляем служебные колонки\n",
        "    cols_to_drop = ['user_id', 'timestamp'] if 'user_id' in X_train.columns else []\n",
        "    X_train = X_train.drop(columns=cols_to_drop, errors='ignore')\n",
        "    \n",
        "    # Заполняем пропуски\n",
        "    X_train = X_train.fillna(0)\n",
        "    \n",
        "    # Обработка тестового датасета (если передан)\n",
        "    if df_test is not None:\n",
        "        test = df_test.copy()\n",
        "        \n",
        "        if 'target' in test.columns:\n",
        "            y_test = test['target'].copy()\n",
        "            X_test = test.drop(columns=['target'], errors='ignore')\n",
        "        else:\n",
        "            y_test = None\n",
        "            X_test = test.copy()\n",
        "        \n",
        "        X_test = X_test.drop(columns=cols_to_drop, errors='ignore')\n",
        "        X_test = X_test.fillna(0)\n",
        "        \n",
        "        # Убеждаемся, что колонки совпадают\n",
        "        missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "        if missing_cols:\n",
        "            for col in missing_cols:\n",
        "                X_test[col] = 0\n",
        "        \n",
        "        X_test = X_test[X_train.columns]  # Приводим к тому же порядку\n",
        "        \n",
        "        return X_train, y_train, X_test, y_test\n",
        "    \n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "\n",
        "# Тестируем pipeline на текущих данных\n",
        "if 'target' in df.columns:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = create_preprocessing_pipeline(train_df, test_df)\n",
        "    \n",
        "    print(f\"\\n Pipeline протестирован:\")\n",
        "    print(f\"   X_train shape: {X_train.shape}\")\n",
        "    print(f\"   X_test shape: {X_test.shape}\")\n",
        "    print(f\"   y_train distribution: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"   y_test distribution: {y_test.value_counts().to_dict()}\")\n",
        "    print(f\"\\n Preprocessing pipeline готов к использованию!\")\n",
        "else:\n",
        "    print(\"\\n Колонка 'target' не найдена, pipeline не протестирован\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Получаем список всех признаков (без target и user_id)\n",
        "feature_cols = [col for col in df.columns if col not in ['target', 'user_id']]\n",
        "\n",
        "print(f\" ФИНАЛЬНЫЙ НАБОР ПРИЗНАКОВ: {len(feature_cols)} признаков\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Группируем признаки по категориям\n",
        "temporal_features = [col for col in feature_cols if any(x in col for x in ['hour', 'day', 'month', 'weekend', 'night', '_sin', '_cos'])]\n",
        "user_features = [col for col in feature_cols if 'user_' in col or 'amount_diff' in col or 'amount_ratio' in col]\n",
        "rolling_features = [col for col in feature_cols if any(x in col for x in ['tx_count', 'tx_mean', 'tx_std', 'tx_sum', 'tx_max'])]\n",
        "statistical_features = [col for col in feature_cols if any(x in col for x in ['zscore', 'percentile', 'ratio', 'cv_amount'])]\n",
        "device_features = [col for col in feature_cols if any(x in col for x in ['device', 'os', 'ip', 'geo'])]\n",
        "behavioral_features = [col for col in feature_cols if any(x in col for x in ['time_since', 'time_until', 'rate', 'interval', 'rapid', 'anomaly', 'morning', 'afternoon', 'evening'])]\n",
        "categorical_features = [col for col in feature_cols if '_freq' in col or '_target_enc' in col]\n",
        "other_features = [col for col in feature_cols if col not in temporal_features + user_features + rolling_features + statistical_features + device_features + behavioral_features + categorical_features]\n",
        "\n",
        "categories = {\n",
        "    'Временные признаки': temporal_features,\n",
        "    'Признаки по пользователю': user_features,\n",
        "    'Rolling-window признаки': rolling_features,\n",
        "    'Статистические признаки': statistical_features,\n",
        "    'Device/OS/IP признаки': device_features,\n",
        "    'Признаки поведения': behavioral_features,\n",
        "    'Категориальные encoding': categorical_features,\n",
        "    'Другие признаки': other_features\n",
        "}\n",
        "\n",
        "# Выводим статистику по категориям\n",
        "print(\"Распределение признаков по категориям:\\n\")\n",
        "for category, cols in categories.items():\n",
        "    if len(cols) > 0:\n",
        "        print(f\"  {category}: {len(cols)} признаков\")\n",
        "\n",
        "print(f\"\\n  ВСЕГО: {len(feature_cols)} признаков\")\n",
        "\n",
        "# Сохраняем список признаков в файл\n",
        "features_list_file = '../data/processed/features_list.txt'\n",
        "with open(features_list_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"ФИНАЛЬНЫЙ НАБОР ПРИЗНАКОВ ДЛЯ ML МОДЕЛИ\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    f.write(f\"Всего признаков: {len(feature_cols)}\\n\\n\")\n",
        "    \n",
        "    for category, cols in categories.items():\n",
        "        if len(cols) > 0:\n",
        "            f.write(f\"{category} ({len(cols)}):\\n\")\n",
        "            for col in cols:\n",
        "                f.write(f\"  - {col}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"\\n Список признаков сохранен в: {features_list_file}\")\n",
        "\n",
        "# Выводим первые 20 признаков для примера\n",
        "print(\"\\nПримеры признаков (первые 20):\")\n",
        "for i, col in enumerate(feature_cols[:20], 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "if len(feature_cols) > 20:\n",
        "    print(f\"  ... и еще {len(feature_cols) - 20} признаков\")\n",
        "\n",
        "print(f\"\\n Финальный набор из {len(feature_cols)} признаков готов для ML модели!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(\"\\n ФИНАЛЬНАЯ СТАТИСТИКА:\")\n",
        "print(f\"  - Всего признаков: {len([c for c in df.columns if c != 'target'])}\")\n",
        "print(f\"  - Размер датасета: {df.shape}\")\n",
        "if 'target' in df.columns:\n",
        "    target_dist = df['target'].value_counts()\n",
        "    print(f\"  - Распределение классов: {target_dist.to_dict()}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
