{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –ë–õ–û–ö 2 ‚Äî DATA ENGINEERING (–ú)\n",
        "## Feature Engineering - –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–ó–∞–¥–∞—á–∏ 29-36)\n",
        "\n",
        "–≠—Ç–æ—Ç notebook —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–∏:\n",
        "\n",
        "- **–ó–∞–¥–∞—á–∞ 29**: –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "- **–ó–∞–¥–∞—á–∞ 30**: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—ã–±—Ä–æ—Å—ã\n",
        "- **–ó–∞–¥–∞—á–∞ 31**: –ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "- **–ó–∞–¥–∞—á–∞ 32**: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å features.csv\n",
        "- **–ó–∞–¥–∞—á–∞ 33**: –°–æ–∑–¥–∞—Ç—å notebook 02_feature_engineering.ipynb (‚úÖ —ç—Ç–æ—Ç —Ñ–∞–π–ª)\n",
        "- **–ó–∞–¥–∞—á–∞ 34**: –ù–∞–ø–∏—Å–∞—Ç—å preprocessing pipeline\n",
        "- **–ó–∞–¥–∞—á–∞ 35**: –ù–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "- **–ó–∞–¥–∞—á–∞ 36**: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** `data/processed/transactions_with_features.csv` (–∏–∑ –∑–∞–¥–∞—á 23-28)  \n",
        "**–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** `data/processed/transactions_with_features_final.csv` (–≥–æ—Ç–æ–≤ –¥–ª—è ML)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç—ã\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞ (–∑–∞–¥–∞—á–∏ 23-28)\n",
        "INPUT_PATH = '../data/processed/transactions_with_features.csv'\n",
        "OUTPUT_PATH = '../data/processed/transactions_with_features_final.csv'\n",
        "PIPELINE_PATH = '../model/preprocessing_pipeline.pkl'\n",
        "\n",
        "print(\"--- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —ç—Ç–∞–ø–∞ ---\")\n",
        "df = pd.read_csv(INPUT_PATH, low_memory=False)\n",
        "print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}\")\n",
        "print(f\"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}\")\n",
        "print(f\"–°—Ç—Ä–æ–∫: {len(df)}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
        "if 'target' in df.columns:\n",
        "    print(f\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "    print(df['target'].value_counts())\n",
        "    print(f\"–î–∏—Å–±–∞–ª–∞–Ω—Å: {df['target'].value_counts()[0] / df['target'].value_counts()[1]:.2f}:1\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 29: –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 29: –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 29: –û–ë–†–ê–ë–û–¢–ö–ê –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –ù–∞—Ö–æ–¥–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"\\n–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(categorical_cols)}\")\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {categorical_cols}\")\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "label_encoders = {}\n",
        "frequency_encodings = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col == 'target' or col == 'user_id':\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–æ–Ω–∫–∏: {col} ---\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {unique_count}\")\n",
        "    \n",
        "    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (>50) - –∏—Å–ø–æ–ª—å–∑—É–µ–º frequency encoding\n",
        "    # –ï—Å–ª–∏ –º–∞–ª–æ - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å label encoding –∏–ª–∏ one-hot\n",
        "    if unique_count > 50:\n",
        "        # Frequency Encoding –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        freq_map = df[col].value_counts() / len(df)\n",
        "        df[f'{col}_freq_encoding'] = df[col].map(freq_map)\n",
        "        frequency_encodings[col] = freq_map\n",
        "        print(f\"  ‚úÖ –°–æ–∑–¥–∞–Ω frequency encoding: {col}_freq_encoding\")\n",
        "        # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "    elif unique_count > 2:\n",
        "        # Label Encoding –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å –Ω–µ–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        le = LabelEncoder()\n",
        "        df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str).fillna('UNKNOWN'))\n",
        "        label_encoders[col] = le\n",
        "        print(f\"  ‚úÖ –°–æ–∑–¥–∞–Ω label encoding: {col}_encoded\")\n",
        "        # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "    else:\n",
        "        # –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ - –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å –∏–ª–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å\n",
        "        df[col] = df[col].astype(str).fillna('UNKNOWN')\n",
        "        print(f\"  ‚úÖ –ë–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –æ—Å—Ç–∞–≤–ª–µ–Ω –∫–∞–∫ –µ—Å—Ç—å\")\n",
        "\n",
        "print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "print(f\"–°–æ–∑–¥–∞–Ω–æ label encoders: {len(label_encoders)}\")\n",
        "print(f\"–°–æ–∑–¥–∞–Ω–æ frequency encodings: {len(frequency_encodings)}\")\n",
        "print(f\"–¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 30: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—ã–±—Ä–æ—Å—ã\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 30: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—ã–±—Ä–æ—Å—ã\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 30: –ü–†–û–í–ï–†–ö–ê –í–´–ë–†–û–°–û–í\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'target' in numeric_cols:\n",
        "    numeric_cols.remove('target')\n",
        "if 'user_id' in numeric_cols:\n",
        "    numeric_cols.remove('user_id')\n",
        "\n",
        "print(f\"\\n–ü—Ä–æ–≤–µ—Ä—è–µ–º {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã...\")\n",
        "\n",
        "# –ú–µ—Ç–æ–¥ IQR –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "outliers_info = {}\n",
        "outliers_count = 0\n",
        "\n",
        "for col in numeric_cols[:20]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 20 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    outlier_count = len(outliers)\n",
        "    \n",
        "    if outlier_count > 0:\n",
        "        outliers_info[col] = {\n",
        "            'count': outlier_count,\n",
        "            'percentage': (outlier_count / len(df)) * 100,\n",
        "            'lower_bound': lower_bound,\n",
        "            'upper_bound': upper_bound\n",
        "        }\n",
        "        outliers_count += outlier_count\n",
        "\n",
        "print(f\"\\nüìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {outliers_count}\")\n",
        "print(f\"–ö–æ–ª–æ–Ω–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏: {len(outliers_info)}\")\n",
        "\n",
        "# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10 –∫–æ–ª–æ–Ω–æ–∫ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "if len(outliers_info) > 0:\n",
        "    sorted_outliers = sorted(outliers_info.items(), key=lambda x: x[1]['count'], reverse=True)[:10]\n",
        "    print(\"\\n–¢–æ–ø-10 –∫–æ–ª–æ–Ω–æ–∫ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏:\")\n",
        "    for col, info in sorted_outliers:\n",
        "        print(f\"  {col}: {info['count']} –≤—ã–±—Ä–æ—Å–æ–≤ ({info['percentage']:.2f}%)\")\n",
        "\n",
        "# –†–µ—à–µ–Ω–∏–µ: –î–ª—è fraud detection –≤—ã–±—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–∂–Ω—ã (–∞–Ω–æ–º–∞–ª–∏–∏ = –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ)\n",
        "# –ü–æ—ç—Ç–æ–º—É –ù–ï —É–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã, –∞ —Ç–æ–ª—å–∫–æ –ª–æ–≥–∏—Ä—É–µ–º –∏—Ö\n",
        "print(\"\\n‚ö†Ô∏è –í–ê–ñ–ù–û: –î–ª—è fraud detection –≤—ã–±—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞\")\n",
        "print(\"–í—ã–±—Ä–æ—Å—ã –ù–ï —É–¥–∞–ª—è—é—Ç—Å—è, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
        "\n",
        "print(\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 31: –ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 31: –ù–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 31: –ù–û–†–ú–ò–†–û–í–ê–ù–ò–ï/–ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# –ò—Å–∫–ª—é—á–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω—É–∂–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å:\n",
        "# - target (—Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è)\n",
        "# - user_id (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)\n",
        "# - –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (0/1)\n",
        "# - —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (sin/cos —É–∂–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ -1 –¥–æ 1)\n",
        "exclude_cols = ['target', 'user_id']\n",
        "exclude_patterns = ['_sin', '_cos', 'is_', '_encoded']  # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∏ –±–∏–Ω–∞—Ä–Ω—ã–µ\n",
        "\n",
        "cols_to_scale = []\n",
        "for col in numeric_cols:\n",
        "    if col in exclude_cols:\n",
        "        continue\n",
        "    if any(pattern in col for pattern in exclude_patterns):\n",
        "        continue\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –±–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (—Ç–æ–ª—å–∫–æ 0 –∏ 1)\n",
        "    unique_vals = df[col].dropna().unique()\n",
        "    if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n",
        "        continue\n",
        "    cols_to_scale.append(col)\n",
        "\n",
        "print(f\"\\n–ù–∞–π–¥–µ–Ω–æ {len(cols_to_scale)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "print(f\"–ü—Ä–∏–º–µ—Ä—ã: {cols_to_scale[:5]}\")\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–µ–º MinMaxScaler –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]\n",
        "# –≠—Ç–æ –ª—É—á—à–µ –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π (LightGBM, XGBoost)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "df_scaled = df.copy()\n",
        "df_scaled[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
        "\n",
        "# –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
        "df = df_scaled\n",
        "\n",
        "print(f\"\\n‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ {len(cols_to_scale)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é MinMaxScaler\")\n",
        "print(\"–ü—Ä–∏–∑–Ω–∞–∫–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1]\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ pipeline\n",
        "print(f\"\\nScaler —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ preprocessing pipeline\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 32: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å features.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 32: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å features.csv\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 32: –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —á–∏—Å–ª–æ–≤—ã–µ (–∫—Ä–æ–º–µ user_id –µ—Å–ª–∏ –æ–Ω –Ω—É–∂–µ–Ω)\n",
        "# –£–¥–∞–ª—è–µ–º user_id –µ—Å–ª–∏ –æ–Ω –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è –º–æ–¥–µ–ª–∏\n",
        "if 'user_id' in df.columns:\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º user_id –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "    user_ids = df['user_id'].copy()\n",
        "    df_for_model = df.drop(columns=['user_id'], errors='ignore')\n",
        "else:\n",
        "    df_for_model = df.copy()\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
        "print(f\"\\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"  –†–∞–∑–º–µ—Ä: {df_for_model.shape}\")\n",
        "print(f\"  –ö–æ–ª–æ–Ω–æ–∫: {len(df_for_model.columns)}\")\n",
        "print(f\"  –°—Ç—Ä–æ–∫: {len(df_for_model)}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ target\n",
        "if 'target' not in df_for_model.columns:\n",
        "    raise ValueError(\"‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç!\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "output_dir = os.path.dirname(OUTPUT_PATH)\n",
        "if output_dir and not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "df_for_model.to_csv(OUTPUT_PATH, index=False)\n",
        "print(f\"\\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_PATH}\")\n",
        "\n",
        "# –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä—Å–∏—é —Å user_id (–µ—Å–ª–∏ –Ω—É–∂–Ω–∞)\n",
        "if 'user_id' in df.columns:\n",
        "    df.to_csv(OUTPUT_PATH.replace('_final.csv', '_with_user_id.csv'), index=False)\n",
        "    print(f\"‚úÖ –í–µ—Ä—Å–∏—è —Å user_id —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {OUTPUT_PATH.replace('_final.csv', '_with_user_id.csv')}\")\n",
        "\n",
        "print(\"\\n‚úÖ –ó–∞–¥–∞—á–∞ 32 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 34: –ù–∞–ø–∏—Å–∞—Ç—å preprocessing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 34: –ù–∞–ø–∏—Å–∞—Ç—å preprocessing pipeline\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 34: –°–û–ó–î–ê–ù–ò–ï PREPROCESSING PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å –¥–ª—è preprocessing pipeline\n",
        "class FraudDetectionPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏.\n",
        "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, label_encoders=None, frequency_encodings=None, scaler=None, feature_columns=None):\n",
        "        self.label_encoders = label_encoders or {}\n",
        "        self.frequency_encodings = frequency_encodings or {}\n",
        "        self.scaler = scaler\n",
        "        self.feature_columns = feature_columns  # –°–ø–∏—Å–æ–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "        \n",
        "    def fit(self, df):\n",
        "        \"\"\"–û–±—É—á–µ–Ω–∏–µ pipeline –Ω–∞ –¥–∞–Ω–Ω—ã—Ö\"\"\"\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target –∏ user_id)\n",
        "        if self.feature_columns is None:\n",
        "            self.feature_columns = [col for col in df.columns \n",
        "                                   if col not in ['target', 'user_id']]\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df):\n",
        "        \"\"\"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∫ –¥–∞–Ω–Ω—ã–º\"\"\"\n",
        "        df_processed = df.copy()\n",
        "        \n",
        "        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "        for col, le in self.label_encoders.items():\n",
        "            if col in df_processed.columns:\n",
        "                df_processed[f'{col}_encoded'] = le.transform(\n",
        "                    df_processed[col].astype(str).fillna('UNKNOWN')\n",
        "                )\n",
        "                df_processed.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        \n",
        "        for col, freq_map in self.frequency_encodings.items():\n",
        "            if col in df_processed.columns:\n",
        "                df_processed[f'{col}_freq_encoding'] = df_processed[col].map(freq_map).fillna(0)\n",
        "                df_processed.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        \n",
        "        # 2. –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "        df_processed.fillna(0, inplace=True)\n",
        "        \n",
        "        # 3. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ scaler –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω)\n",
        "        if self.scaler is not None and self.feature_columns is not None:\n",
        "            available_cols = [col for col in self.feature_columns if col in df_processed.columns]\n",
        "            if available_cols:\n",
        "                df_processed[available_cols] = self.scaler.transform(df_processed[available_cols])\n",
        "        \n",
        "        # 4. –í—ã–±–æ—Ä —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "        if self.feature_columns is not None:\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º target –∏ user_id –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å\n",
        "            keep_cols = self.feature_columns.copy()\n",
        "            if 'target' in df_processed.columns:\n",
        "                keep_cols.append('target')\n",
        "            if 'user_id' in df_processed.columns:\n",
        "                keep_cols.append('user_id')\n",
        "            df_processed = df_processed[[col for col in keep_cols if col in df_processed.columns]]\n",
        "        \n",
        "        return df_processed\n",
        "    \n",
        "    def fit_transform(self, df):\n",
        "        \"\"\"–û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ pipeline\"\"\"\n",
        "        return self.fit(df).transform(df)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º pipeline\n",
        "preprocessor = FraudDetectionPreprocessor(\n",
        "    label_encoders=label_encoders,\n",
        "    frequency_encodings=frequency_encodings,\n",
        "    scaler=scaler,\n",
        "    feature_columns=[col for col in df_for_model.columns if col != 'target']\n",
        ")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º pipeline\n",
        "os.makedirs(os.path.dirname(PIPELINE_PATH), exist_ok=True)\n",
        "joblib.dump(preprocessor, PIPELINE_PATH)\n",
        "print(f\"\\n‚úÖ Preprocessing pipeline —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {PIPELINE_PATH}\")\n",
        "\n",
        "print(\"\\n‚úÖ –ó–∞–¥–∞—á–∞ 34 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 35: –ù–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 35: –ù–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 35: –§–£–ù–ö–¶–ò–Ø –í–ê–õ–ò–î–ê–¶–ò–ò –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def validate_input_data(df, required_features=None, target_col='target'):\n",
        "    \"\"\"\n",
        "    –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "    required_features : list, optional\n",
        "        –°–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "    target_col : str, default='target'\n",
        "        –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "        - 'is_valid': bool - –ø—Ä–æ—à–ª–∞ –ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "        - 'errors': list - —Å–ø–∏—Å–æ–∫ –æ—à–∏–±–æ–∫\n",
        "        - 'warnings': list - —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    warnings = []\n",
        "    \n",
        "    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        errors.append(\"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å pandas.DataFrame\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "    if df.empty:\n",
        "        errors.append(\"DataFrame –ø—É—Å—Ç\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    if required_features is not None:\n",
        "        missing_features = [f for f in required_features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            errors.append(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features[:10]}\")\n",
        "    \n",
        "    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–≤—Å–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º–∏, –∫—Ä–æ–º–µ target –∏ user_id)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    non_numeric_cols = [col for col in df.columns \n",
        "                       if col not in numeric_cols and col not in [target_col, 'user_id']]\n",
        "    if non_numeric_cols:\n",
        "        errors.append(f\"–ù–∞–π–¥–µ–Ω—ã –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∫—Ä–æ–º–µ target/user_id): {non_numeric_cols[:5]}\")\n",
        "    \n",
        "    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    missing_counts = df.isnull().sum()\n",
        "    cols_with_missing = missing_counts[missing_counts > 0]\n",
        "    if len(cols_with_missing) > 0:\n",
        "        warnings.append(f\"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_missing)} –∫–æ–ª–æ–Ω–∫–∞—Ö\")\n",
        "        warnings.append(f\"–ü—Ä–∏–º–µ—Ä—ã: {dict(cols_with_missing.head(5))}\")\n",
        "    \n",
        "    # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()\n",
        "    cols_with_inf = inf_counts[inf_counts > 0]\n",
        "    if len(cols_with_inf) > 0:\n",
        "        errors.append(f\"–ù–∞–π–¥–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_inf)} –∫–æ–ª–æ–Ω–∫–∞—Ö\")\n",
        "    \n",
        "    # 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "    if len(df) == 0:\n",
        "        errors.append(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\")\n",
        "    elif len(df) > 1000000:\n",
        "        warnings.append(f\"–ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {len(df)} —Å—Ç—Ä–æ–∫. –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è\")\n",
        "    \n",
        "    is_valid = len(errors) == 0\n",
        "    \n",
        "    return {\n",
        "        'is_valid': is_valid,\n",
        "        'errors': errors,\n",
        "        'warnings': warnings,\n",
        "        'shape': df.shape,\n",
        "        'numeric_cols_count': len(numeric_cols),\n",
        "        'missing_values_count': missing_counts.sum()\n",
        "    }\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"\\n--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ---\")\n",
        "validation_result = validate_input_data(df_for_model, \n",
        "                                        required_features=[col for col in df_for_model.columns if col != 'target'])\n",
        "\n",
        "print(f\"\\n–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\")\n",
        "print(f\"  –í–∞–ª–∏–¥–Ω–æ: {validation_result['is_valid']}\")\n",
        "print(f\"  –û—à–∏–±–æ–∫: {len(validation_result['errors'])}\")\n",
        "print(f\"  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(validation_result['warnings'])}\")\n",
        "\n",
        "if validation_result['errors']:\n",
        "    print(f\"\\n–û—à–∏–±–∫–∏:\")\n",
        "    for error in validation_result['errors']:\n",
        "        print(f\"  ‚ùå {error}\")\n",
        "\n",
        "if validation_result['warnings']:\n",
        "    print(f\"\\n–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:\")\n",
        "    for warning in validation_result['warnings']:\n",
        "        print(f\"  ‚ö†Ô∏è {warning}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ API\n",
        "validation_code = '''\n",
        "def validate_input_data(df, required_features=None, target_col='target'):\n",
        "    \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è fraud detection –º–æ–¥–µ–ª–∏.\"\"\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    \n",
        "    errors = []\n",
        "    warnings = []\n",
        "    \n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        errors.append(\"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å pandas.DataFrame\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    if df.empty:\n",
        "        errors.append(\"DataFrame –ø—É—Å—Ç\")\n",
        "        return {'is_valid': False, 'errors': errors, 'warnings': warnings}\n",
        "    \n",
        "    if required_features is not None:\n",
        "        missing_features = [f for f in required_features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            errors.append(f\"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing_features[:10]}\")\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    non_numeric_cols = [col for col in df.columns \n",
        "                       if col not in numeric_cols and col not in [target_col, 'user_id']]\n",
        "    if non_numeric_cols:\n",
        "        errors.append(f\"–ù–∞–π–¥–µ–Ω—ã –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {non_numeric_cols[:5]}\")\n",
        "    \n",
        "    missing_counts = df.isnull().sum()\n",
        "    cols_with_missing = missing_counts[missing_counts > 0]\n",
        "    if len(cols_with_missing) > 0:\n",
        "        warnings.append(f\"–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_missing)} –∫–æ–ª–æ–Ω–∫–∞—Ö\")\n",
        "    \n",
        "    inf_counts = np.isinf(df.select_dtypes(include=[np.number])).sum()\n",
        "    cols_with_inf = inf_counts[inf_counts > 0]\n",
        "    if len(cols_with_inf) > 0:\n",
        "        errors.append(f\"–ù–∞–π–¥–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {len(cols_with_inf)} –∫–æ–ª–æ–Ω–∫–∞—Ö\")\n",
        "    \n",
        "    is_valid = len(errors) == 0\n",
        "    \n",
        "    return {\n",
        "        'is_valid': is_valid,\n",
        "        'errors': errors,\n",
        "        'warnings': warnings,\n",
        "        'shape': df.shape\n",
        "    }\n",
        "'''\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤ —Ñ–∞–π–ª\n",
        "validation_file_path = '../src/data_validation.py'\n",
        "os.makedirs(os.path.dirname(validation_file_path), exist_ok=True)\n",
        "with open(validation_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(validation_code)\n",
        "\n",
        "print(f\"\\n‚úÖ –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {validation_file_path}\")\n",
        "print(\"\\n‚úÖ –ó–∞–¥–∞—á–∞ 35 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–¥–∞—á–∞ 36: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–¥–∞—á–∞ 36: –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "print(\"=\"*80)\n",
        "print(\"–ó–ê–î–ê–ß–ê 36: –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target)\n",
        "final_features = [col for col in df_for_model.columns if col != 'target']\n",
        "\n",
        "print(f\"\\nüìä –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í:\")\n",
        "print(f\"  –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(final_features)}\")\n",
        "print(f\"  –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df_for_model.shape}\")\n",
        "\n",
        "# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "feature_categories = {\n",
        "    '–í—Ä–µ–º–µ–Ω–Ω—ã–µ': [f for f in final_features if any(x in f for x in ['hour', 'day', 'month', 'weekend', 'night', 'sin', 'cos'])],\n",
        "    '–ü–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é': [f for f in final_features if 'user_' in f],\n",
        "    'Rolling window': [f for f in final_features if any(x in f for x in ['_1h', '_12h', '_24h'])],\n",
        "    '–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ': [f for f in final_features if any(x in f for x in ['zscore', 'percentile', 'ratio', 'cv'])],\n",
        "    'Device/OS': [f for f in final_features if any(x in f for x in ['device', 'os'])],\n",
        "    '–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ': [f for f in final_features if any(x in f for x in ['time_since', 'time_until', 'tx_rate', 'interval', 'rapid', 'anomaly'])],\n",
        "    '–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ (–∏—Å—Ö–æ–¥–Ω—ã–µ)': [f for f in final_features if '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ' in f or '—Å—Ä–µ–¥–Ω–µ–µ' in f or '–¥–æ–ª—è' in f or '–∏–Ω—Ç–µ—Ä–≤–∞–ª' in f],\n",
        "    '–ü—Ä–æ—á–∏–µ': []\n",
        "}\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "categorized = set()\n",
        "for category, features in feature_categories.items():\n",
        "    categorized.update(features)\n",
        "\n",
        "feature_categories['–ü—Ä–æ—á–∏–µ'] = [f for f in final_features if f not in categorized]\n",
        "\n",
        "print(f\"\\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\")\n",
        "for category, features in feature_categories.items():\n",
        "    if features:\n",
        "        print(f\"  {category}: {len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "        if len(features) <= 5:\n",
        "            print(f\"    –ü—Ä–∏–º–µ—Ä—ã: {features}\")\n",
        "        else:\n",
        "            print(f\"    –ü—Ä–∏–º–µ—Ä—ã: {features[:3]} ... –∏ –µ—â–µ {len(features)-3}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "features_list_path = '../model/final_features_list.txt'\n",
        "with open(features_list_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"–§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø ML –ú–û–î–ï–õ–ò\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    f.write(f\"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(final_features)}\\n\\n\")\n",
        "    for category, features in feature_categories.items():\n",
        "        if features:\n",
        "            f.write(f\"{category} ({len(features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):\\n\")\n",
        "            for feat in features:\n",
        "                f.write(f\"  - {feat}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {features_list_path}\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –≤ pickle –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "import joblib\n",
        "joblib.dump(final_features, '../model/final_features.pkl')\n",
        "print(f\"‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω (pickle): ../model/final_features.pkl\")\n",
        "\n",
        "# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò 29-36 –í–´–ü–û–õ–ù–ï–ù–´!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê:\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 29: –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 30: –í—ã–±—Ä–æ—Å—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 31: –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 32: –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_PATH}\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 33: Notebook —Å–æ–∑–¥–∞–Ω (—ç—Ç–æ—Ç —Ñ–∞–π–ª)\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 34: Preprocessing pipeline —Å–æ–∑–¥–∞–Ω: {PIPELINE_PATH}\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 35: –§—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∞: ../src/data_validation.py\")\n",
        "print(f\"  ‚úÖ –ó–∞–¥–∞—á–∞ 36: –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω: {len(final_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(f\"\\nüéØ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏!\")\n",
        "print(f\"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª: {OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering: –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç:\n",
        "- **–ó–∞–¥–∞—á–∞ 29**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "- **–ó–∞–¥–∞—á–∞ 30**: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "- **–ó–∞–¥–∞—á–∞ 31**: –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤ 01_full_data_pipeline.ipynb)\n",
        "- **–ó–∞–¥–∞—á–∞ 32**: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ features.csv (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤ 01_full_data_pipeline.ipynb)\n",
        "- **–ó–∞–¥–∞—á–∞ 34**: Preprocessing pipeline\n",
        "- **–ó–∞–¥–∞—á–∞ 36**: –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING: –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n",
        "FEATURES_FILE = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(FEATURES_FILE)\n",
        "    print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}\")\n",
        "    print(f\"–ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}\")\n",
        "    print(f\"–°—Ç—Ä–æ–∫: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå –§–∞–π–ª {FEATURES_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    print(\"   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ 01_full_data_pipeline.ipynb –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 29: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\\n\")\n",
        "\n",
        "# –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–µ —á–∏—Å–ª–æ–≤—ã—Ö –∏ –Ω–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö)\n",
        "categorical_cols = []\n",
        "for col in df.columns:\n",
        "    if col in ['target', 'user_id']:\n",
        "        continue\n",
        "    if df[col].dtype == 'object':\n",
        "        categorical_cols.append(col)\n",
        "    elif df[col].dtype in [np.int64, np.int32] and df[col].nunique() < 20:\n",
        "        # –¶–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ç–æ–∂–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏\n",
        "        if col not in ['hour', 'day_of_week', 'day_of_month', 'month']:\n",
        "            categorical_cols.append(col)\n",
        "\n",
        "print(f\"–ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_cols)}\")\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:\")\n",
        "    for i, col in enumerate(categorical_cols[:10], 1):\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"  {i}. {col} ({unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)\")\n",
        "    if len(categorical_cols) > 10:\n",
        "        print(f\"  ... –∏ –µ—â–µ {len(categorical_cols) - 10} –∫–æ–ª–æ–Ω–æ–∫\")\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "processed_categorical = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    df[col].fillna('UNKNOWN', inplace=True)\n",
        "    \n",
        "    unique_count = df[col].nunique()\n",
        "    \n",
        "    # Frequency Encoding (–¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö)\n",
        "    freq_counts = df[col].value_counts()\n",
        "    df[f'{col}_freq'] = df[col].map(freq_counts) / len(df)\n",
        "    processed_categorical.append(f'{col}_freq')\n",
        "    \n",
        "    # Target Encoding (–µ—Å–ª–∏ –µ—Å—Ç—å target) - —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ target –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "    if 'target' in df.columns:\n",
        "        target_mean = df.groupby(col)['target'].mean()\n",
        "        df[f'{col}_target_enc'] = df[col].map(target_mean)\n",
        "        df[f'{col}_target_enc'].fillna(df['target'].mean(), inplace=True)  # –î–ª—è –Ω–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
        "        processed_categorical.append(f'{col}_target_enc')\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É (–ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è encoding)\n",
        "    if unique_count > 10:  # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        df.drop(columns=[col], inplace=True, errors='ignore')\n",
        "        print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω {col}: —Å–æ–∑–¥–∞–Ω frequency encoding, —É–¥–∞–ª–µ–Ω–∞ –∏—Å—Ö–æ–¥–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞\")\n",
        "    else:\n",
        "        print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω {col}: —Å–æ–∑–¥–∞–Ω frequency encoding, –∏—Å—Ö–æ–¥–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞\")\n",
        "\n",
        "print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(categorical_cols)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(f\"–°–æ–∑–¥–∞–Ω–æ {len(processed_categorical)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 30: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ ---\\n\")\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in ['target', 'user_id']]\n",
        "\n",
        "print(f\"–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "\n",
        "# –ú–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "outliers_info = []\n",
        "\n",
        "for col in numeric_cols[:20]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å)\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    \n",
        "    # –ú–µ—Ç–æ–¥ IQR (Interquartile Range)\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    outliers_iqr = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
        "    outliers_pct = (outliers_iqr / len(df)) * 100\n",
        "    \n",
        "    # –ú–µ—Ç–æ–¥ Z-score (–≤—ã–±—Ä–æ—Å—ã > 3 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)\n",
        "    z_scores = np.abs((df[col] - df[col].mean()) / (df[col].std() + 1e-8))\n",
        "    outliers_zscore = (z_scores > 3).sum()\n",
        "    outliers_zscore_pct = (outliers_zscore / len(df)) * 100\n",
        "    \n",
        "    if outliers_iqr > 0 or outliers_zscore > 0:\n",
        "        outliers_info.append({\n",
        "            'column': col,\n",
        "            'outliers_iqr': outliers_iqr,\n",
        "            'outliers_iqr_pct': outliers_pct,\n",
        "            'outliers_zscore': outliers_zscore,\n",
        "            'outliers_zscore_pct': outliers_zscore_pct\n",
        "        })\n",
        "\n",
        "if len(outliers_info) > 0:\n",
        "    outliers_df = pd.DataFrame(outliers_info)\n",
        "    outliers_df = outliers_df.sort_values('outliers_iqr', ascending=False)\n",
        "    \n",
        "    print(\"–¢–û–ü-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(outliers_df.head(10).to_string(index=False))\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    top_5_cols = outliers_df.head(5)['column'].tolist()\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, col in enumerate(top_5_cols[:6]):\n",
        "        if col in df.columns:\n",
        "            # Box plot –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–±—Ä–æ—Å–æ–≤\n",
        "            axes[i].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
        "                          boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
        "            axes[i].set_title(f'{col[:40]}...\\n–í—ã–±—Ä–æ—Å–æ–≤: {outliers_df[outliers_df[\"column\"]==col][\"outliers_iqr\"].values[0]:.0f}', \n",
        "                           fontsize=10, fontweight='bold')\n",
        "            axes[i].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')\n",
        "            axes[i].grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    # –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—à–Ω–∏–µ subplot'—ã\n",
        "    for i in range(len(top_5_cols), 6):\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.suptitle('–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ (—Ç–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—ã–±—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞!\")\n",
        "    print(\"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ù–ï —É–¥–∞–ª—è—Ç—å –≤—ã–±—Ä–æ—Å—ã, –∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, —É—Å—Ç–æ–π—á–∏–≤—ã–µ –∫ –Ω–∏–º\")\n",
        "    print(\"   (–Ω–∞–ø—Ä–∏–º–µ—Ä, Random Forest, XGBoost, LightGBM)\")\n",
        "else:\n",
        "    print(\"‚úÖ –í—ã–±—Ä–æ—Å–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (–∏–ª–∏ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã)\")\n",
        "\n",
        "print(\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 31: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\\n\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω—ã (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1] –∏–ª–∏ –æ–∫–æ–ª–æ —Ç–æ–≥–æ)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [col for col in numeric_cols if col not in ['target', 'user_id']]\n",
        "\n",
        "# –ò—Å–∫–ª—é—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –∏ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "cols_to_check = [col for col in numeric_cols \n",
        "                 if not any(x in col.lower() for x in ['_sin', '_cos', 'is_', 'changed', 'hour', 'day_of_week', 'day_of_month', 'month'])]\n",
        "\n",
        "print(f\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {len(cols_to_check)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "scaling_status = []\n",
        "for col in cols_to_check[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 15\n",
        "    if col in df.columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        mean_val = df[col].mean()\n",
        "        \n",
        "        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, 1] –∏–ª–∏ –±–ª–∏–∑–∫–æ –∫ –Ω–µ–º—É - –≤–µ—Ä–æ—è—Ç–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ\n",
        "        is_scaled = (min_val >= -0.1 and max_val <= 1.1) or (abs(mean_val) < 1 and abs(min_val) < 10 and abs(max_val) < 10)\n",
        "        \n",
        "        scaling_status.append({\n",
        "            'column': col,\n",
        "            'min': min_val,\n",
        "            'max': max_val,\n",
        "            'mean': mean_val,\n",
        "            'likely_scaled': is_scaled\n",
        "        })\n",
        "\n",
        "if len(scaling_status) > 0:\n",
        "    scaling_df = pd.DataFrame(scaling_status)\n",
        "    print(\"–°—Ç–∞—Ç—É—Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(scaling_df.to_string(index=False))\n",
        "    \n",
        "    scaled_count = scaling_df['likely_scaled'].sum()\n",
        "    print(f\"\\n‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–æ: {scaled_count} –∏–∑ {len(scaling_df)} –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\")\n",
        "\n",
        "print(\"\\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–ê–î–ê–ß–ê 32: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å features.csv\n",
        "\n",
        "**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –≠—Ç–∞ –∑–∞–¥–∞—á–∞ —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤ `01_full_data_pipeline.ipynb`.\n",
        "–ó–¥–µ—Å—å –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 32: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ features.csv ---\\n\")\n",
        "\n",
        "FEATURES_OUTPUT = '../data/processed/transactions_with_features.csv'\n",
        "\n",
        "if os.path.exists(FEATURES_OUTPUT):\n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞\n",
        "    file_size = os.path.getsize(FEATURES_OUTPUT) / (1024 * 1024)  # –≤ MB\n",
        "    \n",
        "    print(f\"‚úÖ –§–∞–π–ª features.csv —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {FEATURES_OUTPUT}\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} MB\")\n",
        "    print(f\"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}\")\n",
        "    print(f\"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target): {len([c for c in df.columns if c != 'target'])}\")\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ target\n",
        "    if 'target' in df.columns:\n",
        "        target_dist = df['target'].value_counts()\n",
        "        print(f\"\\n   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ target:\")\n",
        "        for val, count in target_dist.items():\n",
        "            print(f\"     –ö–ª–∞—Å—Å {val}: {count} ({count/len(df)*100:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ –§–∞–π–ª features.csv –≥–æ—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ ML –∏–Ω–∂–∏–Ω–µ—Ä—É!\")\n",
        "else:\n",
        "    print(f\"‚ùå –§–∞–π–ª {FEATURES_OUTPUT} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
        "    print(\"   –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ–π—á–∞—Å...\")\n",
        "    \n",
        "    output_dir = os.path.dirname(FEATURES_OUTPUT)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    df.to_csv(FEATURES_OUTPUT, index=False)\n",
        "    print(f\"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {FEATURES_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## –ó–ê–î–ê–ß–ê 34: –ù–∞–ø–∏—Å–∞—Ç—å preprocessing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 34: –°–æ–∑–¥–∞–Ω–∏–µ preprocessing pipeline ---\\n\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è preprocessing pipeline\n",
        "# –≠—Ç–æ –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è ML –∏–Ω–∂–∏–Ω–µ—Ä–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "def create_preprocessing_pipeline(df_train, df_test=None):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç preprocessing pipeline –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
        "    \n",
        "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
        "    - df_train: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "    - df_test: —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    \n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
        "    - X_train, y_train: –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "    - X_test, y_test: –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ç–µ—Å—Ç–∞ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)\n",
        "    \"\"\"\n",
        "    \n",
        "    # –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "    train = df_train.copy()\n",
        "    \n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
        "    if 'target' in train.columns:\n",
        "        y_train = train['target'].copy()\n",
        "        X_train = train.drop(columns=['target'], errors='ignore')\n",
        "    else:\n",
        "        y_train = None\n",
        "        X_train = train.copy()\n",
        "    \n",
        "    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    cols_to_drop = ['user_id', 'timestamp'] if 'user_id' in X_train.columns else []\n",
        "    X_train = X_train.drop(columns=cols_to_drop, errors='ignore')\n",
        "    \n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    X_train = X_train.fillna(0)\n",
        "    \n",
        "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)\n",
        "    if df_test is not None:\n",
        "        test = df_test.copy()\n",
        "        \n",
        "        if 'target' in test.columns:\n",
        "            y_test = test['target'].copy()\n",
        "            X_test = test.drop(columns=['target'], errors='ignore')\n",
        "        else:\n",
        "            y_test = None\n",
        "            X_test = test.copy()\n",
        "        \n",
        "        X_test = X_test.drop(columns=cols_to_drop, errors='ignore')\n",
        "        X_test = X_test.fillna(0)\n",
        "        \n",
        "        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç\n",
        "        missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "        if missing_cols:\n",
        "            for col in missing_cols:\n",
        "                X_test[col] = 0\n",
        "        \n",
        "        X_test = X_test[X_train.columns]  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ç–æ–º—É –∂–µ –ø–æ—Ä—è–¥–∫—É\n",
        "        \n",
        "        return X_train, y_train, X_test, y_test\n",
        "    \n",
        "    return X_train, y_train\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è pipeline\n",
        "print(\"–°–æ–∑–¥–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è create_preprocessing_pipeline()\")\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ preprocessing pipeline\n",
        "X_train, y_train, X_test, y_test = create_preprocessing_pipeline(train_df, test_df)\n",
        "\n",
        "# –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å:\n",
        "# from lightgbm import LGBMClassifier\n",
        "# model = LGBMClassifier()\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "\"\"\")\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º pipeline –Ω–∞ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "if 'target' in df.columns:\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = create_preprocessing_pipeline(train_df, test_df)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Pipeline –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω:\")\n",
        "    print(f\"   X_train shape: {X_train.shape}\")\n",
        "    print(f\"   X_test shape: {X_test.shape}\")\n",
        "    print(f\"   y_train distribution: {y_train.value_counts().to_dict()}\")\n",
        "    print(f\"   y_test distribution: {y_test.value_counts().to_dict()}\")\n",
        "    print(f\"\\n‚úÖ Preprocessing pipeline –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, pipeline –Ω–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
        "\n",
        "print(\"\\n‚úÖ –ó–∞–¥–∞—á–∞ 34 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞: preprocessing pipeline —Å–æ–∑–¥–∞–Ω\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- –ó–ê–î–ê–ß–ê 36: –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---\\n\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target –∏ user_id)\n",
        "feature_cols = [col for col in df.columns if col not in ['target', 'user_id']]\n",
        "\n",
        "print(f\"üìä –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í: {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "temporal_features = [col for col in feature_cols if any(x in col for x in ['hour', 'day', 'month', 'weekend', 'night', '_sin', '_cos'])]\n",
        "user_features = [col for col in feature_cols if 'user_' in col or 'amount_diff' in col or 'amount_ratio' in col]\n",
        "rolling_features = [col for col in feature_cols if any(x in col for x in ['tx_count', 'tx_mean', 'tx_std', 'tx_sum', 'tx_max'])]\n",
        "statistical_features = [col for col in feature_cols if any(x in col for x in ['zscore', 'percentile', 'ratio', 'cv_amount'])]\n",
        "device_features = [col for col in feature_cols if any(x in col for x in ['device', 'os', 'ip', 'geo'])]\n",
        "behavioral_features = [col for col in feature_cols if any(x in col for x in ['time_since', 'time_until', 'rate', 'interval', 'rapid', 'anomaly', 'morning', 'afternoon', 'evening'])]\n",
        "categorical_features = [col for col in feature_cols if '_freq' in col or '_target_enc' in col]\n",
        "other_features = [col for col in feature_cols if col not in temporal_features + user_features + rolling_features + statistical_features + device_features + behavioral_features + categorical_features]\n",
        "\n",
        "categories = {\n",
        "    '–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏': temporal_features,\n",
        "    '–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é': user_features,\n",
        "    'Rolling-window –ø—Ä–∏–∑–Ω–∞–∫–∏': rolling_features,\n",
        "    '–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏': statistical_features,\n",
        "    'Device/OS/IP –ø—Ä–∏–∑–Ω–∞–∫–∏': device_features,\n",
        "    '–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è': behavioral_features,\n",
        "    '–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ encoding': categorical_features,\n",
        "    '–î—Ä—É–≥–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏': other_features\n",
        "}\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "print(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\\n\")\n",
        "for category, cols in categories.items():\n",
        "    if len(cols) > 0:\n",
        "        print(f\"  {category}: {len(cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "print(f\"\\n  –í–°–ï–ì–û: {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ñ–∞–π–ª\n",
        "features_list_file = '../data/processed/features_list.txt'\n",
        "with open(features_list_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"–§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø ML –ú–û–î–ï–õ–ò\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    f.write(f\"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}\\n\\n\")\n",
        "    \n",
        "    for category, cols in categories.items():\n",
        "        if len(cols) > 0:\n",
        "            f.write(f\"{category} ({len(cols)}):\\n\")\n",
        "            for col in cols:\n",
        "                f.write(f\"  - {col}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {features_list_file}\")\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–µ—Ä–≤—ã–µ 20):\")\n",
        "for i, col in enumerate(feature_cols[:20], 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "if len(feature_cols) > 20:\n",
        "    print(f\"  ... –∏ –µ—â–µ {len(feature_cols) - 20} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "print(f\"\\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –∏–∑ {len(feature_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≥–æ—Ç–æ–≤ –¥–ª—è ML –º–æ–¥–µ–ª–∏!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ü–û FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n‚úÖ –í–´–ü–û–õ–ù–ï–ù–ù–´–ï –ó–ê–î–ê–ß–ò:\")\n",
        "print(\"  29. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  30. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  31. –ù–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ/–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ - –í–´–ü–û–õ–ù–ï–ù–û (–≤ 01_full_data_pipeline.ipynb)\")\n",
        "print(\"  32. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ features.csv - –í–´–ü–û–õ–ù–ï–ù–û (–≤ 01_full_data_pipeline.ipynb)\")\n",
        "print(\"  33. –°–æ–∑–¥–∞–Ω–∏–µ notebook 02_feature_engineering.ipynb - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  34. Preprocessing pipeline - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "print(\"  36. –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - –í–´–ü–û–õ–ù–ï–ù–û\")\n",
        "\n",
        "print(\"\\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
        "print(f\"  - –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len([c for c in df.columns if c != 'target'])}\")\n",
        "print(f\"  - –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}\")\n",
        "if 'target' in df.columns:\n",
        "    target_dist = df['target'].value_counts()\n",
        "    print(f\"  - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {target_dist.to_dict()}\")\n",
        "\n",
        "print(\"\\nüìÅ –§–ê–ô–õ–´ –î–õ–Ø ML –ò–ù–ñ–ò–ù–ï–†–ê:\")\n",
        "print(\"  - data/processed/transactions_with_features.csv - —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\")\n",
        "print(\"  - data/processed/features_list.txt - —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(\"  - FEATURES_DOCUMENTATION.md - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º\")\n",
        "print(\"  - notebooks/02_feature_engineering.ipynb - preprocessing pipeline\")\n",
        "\n",
        "print(\"\\n‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò FEATURE ENGINEERING –í–´–ü–û–õ–ù–ï–ù–´!\")\n",
        "print(\"   –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ ML –∏–Ω–∂–∏–Ω–µ—Ä—É. üöÄ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
