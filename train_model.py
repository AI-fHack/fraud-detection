"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π fraud detection
–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤—Å–µ –∑–∞–¥–∞—á–∏ –∏–∑ 03_final_model.ipynb
"""
import sys
import io
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –±–µ–∑ GUI
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
warnings.filterwarnings('ignore')

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    precision_score, recall_score, f1_score, fbeta_score,
    confusion_matrix, classification_report, roc_auc_score, roc_curve,
    make_scorer
)
from sklearn.preprocessing import StandardScaler
import joblib

# Gradient Boosting
import xgboost as xgb
import lightgbm as lgb
# CatBoost –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω - –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
# –ï—Å–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—Ç—Ä–µ–±—É–µ—Ç Visual Studio), –∫–æ–¥ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç —Ä–∞–±–æ—Ç—É –±–µ–∑ –Ω–µ–≥–æ
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("WARNING: CatBoost not installed (requires Visual Studio). Continuing without it.")
    print("NOTE: If CatBoost is installed later, it will be automatically used without code changes.")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
import optuna

# –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
import shap

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("‚úÖ –í—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")

# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
print("="*80)

DATA_PATH = 'data/processed/transactions_with_features.csv'

df = pd.read_csv(DATA_PATH)
print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}")
print(f"\n–ö–æ–ª–æ–Ω–∫–∏: {len(df.columns)}")
print(f"–ü–µ—Ä–≤—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫: {list(df.columns[:5])}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
if 'target' in df.columns:
    print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    print(df['target'].value_counts())
    imbalance_ratio = df['target'].value_counts()[0] / df['target'].value_counts()[1]
    print(f"\n–î–∏—Å–±–∞–ª–∞–Ω—Å: {imbalance_ratio:.2f}:1")
else:
    raise ValueError("‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'target' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
X = df.drop(columns=['target'], errors='ignore')
y = df['target']

# –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
non_numeric_cols = X.select_dtypes(include=['object']).columns
if len(non_numeric_cols) > 0:
    print(f"\n‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(non_numeric_cols)}")
    X = X.select_dtypes(include=[np.number])

# –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
nan_count = X.isna().sum().sum()
if nan_count > 0:
    print(f"\n‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π. –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏.")
    X = X.fillna(0)

print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 37: TRAIN/TEST SPLIT (STRATIFIED)
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 37: TRAIN/TEST SPLIT (STRATIFIED)")
print("="*80)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42, 
    stratify=y  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤
)

print(f"‚úÖ Train/Test Split –≤—ã–ø–æ–ª–Ω–µ–Ω:")
print(f"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)")
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:")
print(y_train.value_counts())
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ test:")
print(y_test.value_counts())

# ============================================================================
# –ó–ê–î–ê–ß–ê 38: BASELINE –ú–û–î–ï–õ–¨ LOGISTIC REGRESSION
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 38: BASELINE –ú–û–î–ï–õ–¨ LOGISTIC REGRESSION")
print("="*80)

print("--- –û–±—É—á–µ–Ω–∏–µ Baseline –º–æ–¥–µ–ª–∏ (Logistic Regression) ---")

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Logistic Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –û–±—É—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤
baseline_model = LogisticRegression(
    random_state=42,
    max_iter=1000,
    class_weight='balanced'  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
)

baseline_model.fit(X_train_scaled, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_baseline = baseline_model.predict(X_test_scaled)
y_pred_proba_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]

print("‚úÖ Baseline –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")

# ============================================================================
# –ó–ê–î–ê–ß–ê 39: –ú–ï–¢–†–ò–ö–ò PRECISION, RECALL, F2
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 39: –ú–ï–¢–†–ò–ö–ò PRECISION, RECALL, F2")
print("="*80)

def calculate_metrics(y_true, y_pred, y_pred_proba=None, model_name=""):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    f2 = fbeta_score(y_true, y_pred, beta=2)  # F2-score (–±–æ–ª—å—à–∏–π –≤–µ—Å Recall)
    
    metrics = {
        'Model': model_name,
        'Precision': precision,
        'Recall': recall,
        'F1-score': f1,
        'F2-score': f2
    }
    
    if y_pred_proba is not None:
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        metrics['ROC-AUC'] = roc_auc
    
    return metrics

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Baseline
baseline_metrics = calculate_metrics(
    y_test, y_pred_baseline, y_pred_proba_baseline, "Baseline (Logistic Regression)"
)

print("üìä –ú–ï–¢–†–ò–ö–ò BASELINE –ú–û–î–ï–õ–ò:")
print("=" * 60)
for key, value in baseline_metrics.items():
    if key != 'Model':
        print(f"{key:15s}: {value:.4f}")
print("=" * 60)

# ============================================================================
# –ó–ê–î–ê–ß–ê 40: CONFUSION MATRIX
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 40: CONFUSION MATRIX")
print("="*80)

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
os.makedirs('model', exist_ok=True)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Confusion Matrix
cm_baseline = confusion_matrix(y_test, y_pred_baseline)
sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'],
            yticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'])
axes[0].set_title('Confusion Matrix - Baseline (Logistic Regression)', fontsize=12, fontweight='bold')
axes[0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
axes[0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_baseline)
axes[1].plot(fpr, tpr, label=f'Baseline (AUC = {baseline_metrics["ROC-AUC"]:.3f})', linewidth=2)
axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')
axes[1].set_xlabel('False Positive Rate')
axes[1].set_ylabel('True Positive Rate')
axes[1].set_title('ROC Curve - Baseline', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('model/baseline_metrics.png', dpi=300, bbox_inches='tight')
plt.close()

print("\n‚úÖ Baseline –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")

# ============================================================================
# –ó–ê–î–ê–ß–ò 41-44: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ò 41-44: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
print("="*80)

# –ó–∞–¥–∞—á–∞ 41: RandomForest
print("\n--- –û–±—É—á–µ–Ω–∏–µ RandomForest ---")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]
print("‚úÖ RandomForest –æ–±—É—á–µ–Ω!")

# –ó–∞–¥–∞—á–∞ 42: XGBoost
print("\n--- –û–±—É—á–µ–Ω–∏–µ XGBoost ---")
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
    random_state=42,
    eval_metric='logloss',
    n_jobs=-1
)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]
print("‚úÖ XGBoost –æ–±—É—á–µ–Ω!")

# –ó–∞–¥–∞—á–∞ 43: LightGBM
print("\n--- –û–±—É—á–µ–Ω–∏–µ LightGBM ---")
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1,
    verbose=-1
)
lgb_model.fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)
y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]
print("‚úÖ LightGBM –æ–±—É—á–µ–Ω!")

# –ó–∞–¥–∞—á–∞ 44: CatBoost
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∏—Ç—Å—è, –µ—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
# –ù–∏–∫–∞–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ CatBoost
if CATBOOST_AVAILABLE:
    print("\n--- –û–±—É—á–µ–Ω–∏–µ CatBoost ---")
    cb_model = cb.CatBoostClassifier(
        iterations=100,
        depth=6,
        learning_rate=0.1,
        class_weights=[1, len(y_train[y_train==0]) / len(y_train[y_train==1])],  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
        random_state=42,
        verbose=False
    )
    cb_model.fit(X_train, y_train)
    y_pred_cb = cb_model.predict(X_test)
    y_pred_proba_cb = cb_model.predict_proba(X_test)[:, 1]
    print("‚úÖ CatBoost –æ–±—É—á–µ–Ω!")
else:
    print("\n--- CatBoost –ø—Ä–æ–ø—É—â–µ–Ω (–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω) ---")
    print("   –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ CatBoost (—Ç—Ä–µ–±—É–µ—Ç Visual Studio)")
    print("   –∫–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∏—Ç CatBoost –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
    y_pred_cb = None
    y_pred_proba_cb = None

# ============================================================================
# –ó–ê–î–ê–ß–ê 45: –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ü–û –ú–ï–¢–†–ò–ö–ê–ú
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 45: –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ü–û –ú–ï–¢–†–ò–ö–ê–ú")
print("="*80)

all_models = {
    'Baseline (Logistic Regression)': (y_pred_baseline, y_pred_proba_baseline),
    'RandomForest': (y_pred_rf, y_pred_proba_rf),
    'XGBoost': (y_pred_xgb, y_pred_proba_xgb),
    'LightGBM': (y_pred_lgb, y_pred_proba_lgb),
}
if CATBOOST_AVAILABLE and y_pred_cb is not None:
    all_models['CatBoost'] = (y_pred_cb, y_pred_proba_cb)

results = []
for name, (y_pred, y_pred_proba) in all_models.items():
    metrics = calculate_metrics(y_test, y_pred, y_pred_proba, name)
    results.append(metrics)

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('F2-score', ascending=False)

print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
print("=" * 80)
print(results_df.to_string(index=False))
print("=" * 80)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

metrics_to_plot = ['Precision', 'Recall', 'F1-score', 'F2-score']
for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx // 2, idx % 2]
    bars = ax.barh(results_df['Model'], results_df[metric], color=sns.color_palette("husl", len(results_df)))
    ax.set_xlabel(metric, fontsize=11, fontweight='bold')
    ax.set_title(f'{metric} –ø–æ –º–æ–¥–µ–ª—è–º', fontsize=12, fontweight='bold')
    ax.grid(alpha=0.3, axis='x')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (bar, value) in enumerate(zip(bars, results_df[metric])):
        ax.text(value, i, f' {value:.4f}', va='center', fontweight='bold')

plt.tight_layout()
plt.savefig('model/models_comparison.png', dpi=300, bbox_inches='tight')
plt.close()

# ============================================================================
# –ó–ê–î–ê–ß–ê 46: –í–´–ë–û–† –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 46: –í–´–ë–û–† –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò")
print("="*80)

best_model_name = results_df.iloc[0]['Model']
best_f2_score = results_df.iloc[0]['F2-score']

print(f"üèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
print(f"   F2-score: {best_f2_score:.4f}")
print(f"\nüìã –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏:")
best_metrics = results_df[results_df['Model'] == best_model_name].iloc[0]
for metric in ['Precision', 'Recall', 'F1-score', 'F2-score', 'ROC-AUC']:
    if metric in best_metrics:
        print(f"   {metric}: {best_metrics[metric]:.4f}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
if best_model_name == 'Baseline (Logistic Regression)':
    best_model = baseline_model
    best_scaler = scaler
elif best_model_name == 'RandomForest':
    best_model = rf_model
    best_scaler = None
elif best_model_name == 'XGBoost':
    best_model = xgb_model
    best_scaler = None
elif best_model_name == 'LightGBM':
    best_model = lgb_model
    best_scaler = None
elif best_model_name == 'CatBoost' and CATBOOST_AVAILABLE:
    best_model = cb_model
    best_scaler = None

print(f"\n‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –≤—ã–±—Ä–∞–Ω–∞: {best_model_name}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 47: –ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í (OPTUNA)
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 47: –ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í (OPTUNA)")
print("="*80)

print("--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna ---")
print("‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è...")

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å (–±–µ—Ä–µ–º –ª—É—á—à—É—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞)
# –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º LightGBM (–æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)

def objective(trial):
    """Objective —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna"""
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'class_weight': 'balanced',
        'random_state': 42,
        'n_jobs': -1,
        'verbose': -1
    }
    
    model = lgb.LGBMClassifier(**params)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º cross-validation –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    f2_scorer = make_scorer(fbeta_score, beta=2)
    scores = cross_val_score(model, X_train, y_train, cv=cv, 
                            scoring=f2_scorer, n_jobs=-1)
    
    return scores.mean()

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20, show_progress_bar=True)

print(f"\n‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
print(f"–õ—É—á—à–∏–π F2-score (CV): {study.best_value:.4f}")
print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
best_params = study.best_params.copy()
best_params.update({
    'class_weight': 'balanced',
    'random_state': 42,
    'n_jobs': -1,
    'verbose': -1
})

optimized_model = lgb.LGBMClassifier(**best_params)
optimized_model.fit(X_train, y_train)
y_pred_optimized = optimized_model.predict(X_test)
y_pred_proba_optimized = optimized_model.predict_proba(X_test)[:, 1]

optimized_metrics = calculate_metrics(
    y_test, y_pred_optimized, y_pred_proba_optimized, "LightGBM (Optimized)"
)
print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
for key, value in optimized_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# ============================================================================
# –ó–ê–î–ê–ß–ê 48: –£–õ–£–ß–®–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í (–û–¢–ë–û–† FEATURE IMPORTANCE)
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 48: –£–õ–£–ß–®–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í (–û–¢–ë–û–† FEATURE IMPORTANCE)")
print("="*80)

print("--- –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ ---")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º feature importance –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': optimized_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\n–¢–æ–ø-20 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
print(feature_importance.head(20).to_string(index=False))

# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ø 80%)
importance_threshold = feature_importance['importance'].quantile(0.2)
selected_features = feature_importance[feature_importance['importance'] >= importance_threshold]['feature'].tolist()

print(f"\n‚úÖ –û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(X_train.columns)}")
print(f"–ü–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏: {importance_threshold:.6f}")

# –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

optimized_model_selected = lgb.LGBMClassifier(**best_params)
optimized_model_selected.fit(X_train_selected, y_train)
y_pred_selected = optimized_model_selected.predict(X_test_selected)
y_pred_proba_selected = optimized_model_selected.predict_proba(X_test_selected)[:, 1]

selected_metrics = calculate_metrics(
    y_test, y_pred_selected, y_pred_proba_selected, "LightGBM (Optimized + Feature Selection)"
)
print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
for key, value in selected_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
plt.figure(figsize=(10, 8))
top_features = feature_importance.head(20)
plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')
plt.title('Top-20 Feature Importance', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.grid(alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('model/feature_importance.png', dpi=300, bbox_inches='tight')
plt.close()

# ============================================================================
# –ó–ê–î–ê–ß–ê 49: CROSS-VALIDATION
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 49: CROSS-VALIDATION")
print("="*80)

print("--- Cross-Validation –æ—Ü–µ–Ω–∫–∞ ---")

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# CV –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
f2_scorer = make_scorer(fbeta_score, beta=2)
cv_scores_f2 = cross_val_score(
    optimized_model_selected, X_train_selected, y_train, 
    cv=cv, scoring=f2_scorer, n_jobs=-1
)
cv_scores_recall = cross_val_score(
    optimized_model_selected, X_train_selected, y_train,
    cv=cv, scoring='recall', n_jobs=-1
)
cv_scores_precision = cross_val_score(
    optimized_model_selected, X_train_selected, y_train,
    cv=cv, scoring='precision', n_jobs=-1
)

print(f"\nüìä Cross-Validation —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (5-fold):")
print(f"  F2-score: {cv_scores_f2.mean():.4f} (+/- {cv_scores_f2.std() * 2:.4f})")
print(f"  Recall:   {cv_scores_recall.mean():.4f} (+/- {cv_scores_recall.std() * 2:.4f})")
print(f"  Precision: {cv_scores_precision.mean():.4f} (+/- {cv_scores_precision.std() * 2:.4f})")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è CV —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
fig, ax = plt.subplots(figsize=(10, 6))
cv_results = pd.DataFrame({
    'Fold': range(1, 6),
    'F2-score': cv_scores_f2,
    'Recall': cv_scores_recall,
    'Precision': cv_scores_precision
})
cv_results.plot(x='Fold', y=['F2-score', 'Recall', 'Precision'], 
                kind='bar', ax=ax, color=['steelblue', 'coral', 'green'])
ax.set_ylabel('Score', fontsize=11, fontweight='bold')
ax.set_title('Cross-Validation Results (5-fold)', fontsize=12, fontweight='bold')
ax.set_xticklabels(cv_results['Fold'], rotation=0)
ax.legend(loc='best')
ax.grid(alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig('model/cv_results.png', dpi=300, bbox_inches='tight')
plt.close()

# ============================================================================
# –ó–ê–î–ê–ß–ê 50: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–û–†–û–ì–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 50: –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–û–†–û–ì–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
print("="*80)

print("--- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ---")

# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç F2-score
thresholds = np.arange(0.1, 0.9, 0.05)
f2_scores = []
recall_scores = []
precision_scores = []

for threshold in thresholds:
    y_pred_thresh = (y_pred_proba_selected >= threshold).astype(int)
    f2 = fbeta_score(y_test, y_pred_thresh, beta=2)
    rec = recall_score(y_test, y_pred_thresh)
    prec = precision_score(y_test, y_pred_thresh)
    
    f2_scores.append(f2)
    recall_scores.append(rec)
    precision_scores.append(prec)

# –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
best_threshold_idx = np.argmax(f2_scores)
best_threshold = thresholds[best_threshold_idx]
best_f2_thresh = f2_scores[best_threshold_idx]

print(f"\n‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_threshold:.3f}")
print(f"   F2-score –ø—Ä–∏ —ç—Ç–æ–º –ø–æ—Ä–æ–≥–µ: {best_f2_thresh:.4f}")

# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
y_pred_optimal = (y_pred_proba_selected >= best_threshold).astype(int)
optimal_metrics = calculate_metrics(
    y_test, y_pred_optimal, y_pred_proba_selected, 
    f"LightGBM (Optimized + Feature Selection + Optimal Threshold={best_threshold:.3f})"
)

print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º:")
for key, value in optimal_metrics.items():
    if key != 'Model':
        print(f"  {key}: {value:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(thresholds, f2_scores, label='F2-score', linewidth=2, marker='o')
ax.plot(thresholds, recall_scores, label='Recall', linewidth=2, marker='s')
ax.plot(thresholds, precision_scores, label='Precision', linewidth=2, marker='^')
ax.axvline(best_threshold, color='red', linestyle='--', 
           label=f'Optimal Threshold = {best_threshold:.3f}')
ax.set_xlabel('Threshold', fontsize=11, fontweight='bold')
ax.set_ylabel('Score', fontsize=11, fontweight='bold')
ax.set_title('Threshold Optimization', fontsize=12, fontweight='bold')
ax.legend()
ax.grid(alpha=0.3)
plt.tight_layout()
plt.savefig('model/threshold_optimization.png', dpi=300, bbox_inches='tight')
plt.close()

# –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
final_model = optimized_model_selected
final_threshold = best_threshold

# ============================================================================
# –ó–ê–î–ê–ß–ê 51: –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ê 51: –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò")
print("="*80)

final_metrics = optimal_metrics
print("üìä –ò–¢–û–ì–û–í–´–ï –ú–ï–¢–†–ò–ö–ò –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò")
print("=" * 80)
for key, value in final_metrics.items():
    if key != 'Model':
        print(f"{key:15s}: {value:.4f}")
print("=" * 80)

# Confusion Matrix —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

cm_final = confusion_matrix(y_test, y_pred_optimal)
sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'],
            yticklabels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ'])
axes[0].set_title('Confusion Matrix - Final Model', fontsize=12, fontweight='bold')
axes[0].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
axes[0].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_selected)
axes[1].plot(fpr, tpr, label=f'Final Model (AUC = {final_metrics["ROC-AUC"]:.3f})', 
             linewidth=2, color='steelblue')
axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')
axes[1].set_xlabel('False Positive Rate')
axes[1].set_ylabel('True Positive Rate')
axes[1].set_title('ROC Curve - Final Model', fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('model/final_model_metrics.png', dpi=300, bbox_inches='tight')
plt.close()

print("\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥–æ—Ç–æ–≤—ã!")

# ============================================================================
# –ó–ê–î–ê–ß–ò 52-56: SHAP –ê–ù–ê–õ–ò–ó
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ò 52-56: SHAP –ê–ù–ê–õ–ò–ó")
print("="*80)

print("--- SHAP Analysis ---")
print("‚úÖ SHAP —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SHAP explainer
# –î–ª—è LightGBM –∏—Å–ø–æ–ª—å–∑—É–µ–º TreeExplainer (–±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ)
explainer = shap.TreeExplainer(final_model)
shap_values = explainer.shap_values(X_test_selected)

# –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ shap_values - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ [values_class_0, values_class_1]
# –ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∫–ª–∞—Å—Å 1 (–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ)
if isinstance(shap_values, list):
    shap_values_fraud = shap_values[1]  # –ó–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∞ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
else:
    shap_values_fraud = shap_values

print(f"‚úÖ SHAP –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω—ã –¥–ª—è {len(X_test_selected)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")

# –ó–∞–¥–∞—á–∞ 53: SHAP summary plot
print("\n--- –°–æ–∑–¥–∞–Ω–∏–µ SHAP Summary Plot ---")
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values_fraud, X_test_selected, 
                  feature_names=selected_features,
                  show=False, max_display=20)
plt.title('SHAP Summary Plot (Top 20 Features)', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('model/shap_summary_plot.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ SHAP Summary Plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")

# –ó–∞–¥–∞—á–∞ 54: SHAP bar importance
print("\n--- –°–æ–∑–¥–∞–Ω–∏–µ SHAP Bar Plot ---")
plt.figure(figsize=(10, 8))
# –°–æ–∑–¥–∞–µ–º Explanation –æ–±—ä–µ–∫—Ç –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
shap_explanation = shap.Explanation(
    values=shap_values_fraud,
    base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,
    data=X_test_selected.values,
    feature_names=selected_features
)
shap.plots.bar(shap_explanation, max_display=20, show=False)
plt.title('SHAP Feature Importance (Bar Plot)', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('model/shap_bar_plot.png', dpi=300, bbox_inches='tight')
plt.close()
print("‚úÖ SHAP Bar Plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")

# –ó–∞–¥–∞—á–∞ 55: –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
print("\n--- –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ ---")

# –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
fraud_indices = y_test[y_test == 1].index
if len(fraud_indices) > 0:
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
    example_idx = fraud_indices[0]
    example_idx_in_test = list(y_test.index).index(example_idx)
    
    print(f"\nüìã –ü—Ä–∏–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ #{example_idx_in_test}:")
    print(f"   –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è (1)")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {y_pred_proba_selected[example_idx_in_test]:.4f}")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {'–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è' if y_pred_optimal[example_idx_in_test] == 1 else '–ù–æ—Ä–º–∞–ª—å–Ω–∞—è'}")
    
    # Waterfall plot –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    plt.figure(figsize=(10, 8))
    shap.waterfall_plot(
        shap.Explanation(
            values=shap_values_fraud[example_idx_in_test],
            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,
            data=X_test_selected.iloc[example_idx_in_test].values,
            feature_names=selected_features
        ),
        max_display=15,
        show=False
    )
    plt.title(f'SHAP Waterfall Plot - Transaction #{example_idx_in_test}\n(Predicted: {y_pred_proba_selected[example_idx_in_test]:.4f})', 
              fontsize=12, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('model/shap_waterfall_example.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print("\n‚úÖ –ü—Ä–∏–º–µ—Ä –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω!")
else:
    print("‚ö†Ô∏è –í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ—Ç –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞")
    
    # –ë–µ—Ä–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
    high_prob_idx = np.argmax(y_pred_proba_selected)
    print(f"\nüìã –ü—Ä–∏–º–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞:")
    print(f"   –ò–Ω–¥–µ–∫—Å: {high_prob_idx}")
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {y_pred_proba_selected[high_prob_idx]:.4f}")
    
    plt.figure(figsize=(10, 8))
    shap.waterfall_plot(
        shap.Explanation(
            values=shap_values_fraud[high_prob_idx],
            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,
            data=X_test_selected.iloc[high_prob_idx].values,
            feature_names=selected_features
        ),
        max_display=15,
        show=False
    )
    plt.title(f'SHAP Waterfall Plot - High Risk Transaction\n(Predicted: {y_pred_proba_selected[high_prob_idx]:.4f})', 
              fontsize=12, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.savefig('model/shap_waterfall_example.png', dpi=300, bbox_inches='tight')
    plt.close()

print("\n‚úÖ –í—Å–µ SHAP –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
print("  - shap_summary_plot.png")
print("  - shap_bar_plot.png")
print("  - shap_waterfall_example.png")

# ============================================================================
# –ó–ê–î–ê–ß–ò 57-58: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø
# ============================================================================
print("\n" + "="*80)
print("–ó–ê–î–ê–ß–ò 57-58: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø")
print("="*80)

# –ó–∞–¥–∞—á–∞ 57: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –≤ model.pkl
print("\n--- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å, —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø–æ—Ä–æ–≥
model_package = {
    'model': final_model,
    'selected_features': selected_features,
    'threshold': final_threshold,
    'scaler': None,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è LightGBM
    'model_type': 'LightGBM',
    'metrics': final_metrics
}

joblib.dump(model_package, 'model/model.pkl')
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model/model.pkl")

# –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
joblib.dump(final_model, 'model/final_model.pkl')
joblib.dump(selected_features, 'model/selected_features.pkl')
joblib.dump(final_threshold, 'model/threshold.pkl')

print("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
print("  - final_model.pkl")
print("  - selected_features.pkl")
print("  - threshold.pkl")

# –ó–∞–¥–∞—á–∞ 58: –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ö–æ–¥/–≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
print("\n--- –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ---")

model_doc = f"""# üìã –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ Fraud Detection

## –ú–æ–¥–µ–ª—å
- **–¢–∏–ø:** {model_package['model_type']}
- **–í–µ—Ä—Å–∏—è:** 1.0
- **–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

## –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- **F2-score:** {final_metrics['F2-score']:.4f}
- **Recall:** {final_metrics['Recall']:.4f}
- **Precision:** {final_metrics['Precision']:.4f}
- **F1-score:** {final_metrics['F1-score']:.4f}
- **ROC-AUC:** {final_metrics['ROC-AUC']:.4f}

## –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- **Threshold:** {final_threshold:.3f}

## –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Input)
–ú–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ DataFrame —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ ({len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):

{chr(10).join([f"- {feat}" for feat in selected_features[:20]])}
... –∏ –µ—â–µ {len(selected_features) - 20} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º–∏
- –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω—ã (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0)
- –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –≤ —Å–ø–∏—Å–∫–µ selected_features

## –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (Output)
–ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
1. **–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞** (float): –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1
   - –ë–ª–∏–∑–∫–æ –∫ 0: –Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
   - –ë–ª–∏–∑–∫–æ –∫ 1: –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞

2. **–ö–ª–∞—Å—Å** (int): 0 –∏–ª–∏ 1
   - 0: –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è
   - 1: –ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è
   - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å –ø–æ—Ä–æ–≥–æ–º ({final_threshold:.3f})

## –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```python
import joblib
import pandas as pd

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_package = joblib.load('model/model.pkl')
model = model_package['model']
selected_features = model_package['selected_features']
threshold = model_package['threshold']

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# X_new –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑ selected_features
X_new = pd.DataFrame(...)  # –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
probabilities = model.predict_proba(X_new[selected_features])[:, 1]
predictions = (probabilities >= threshold).astype(int)

# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
for i, (prob, pred) in enumerate(zip(probabilities, predictions)):
    print(f"–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è {{i}}: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å={{prob:.4f}}, –∫–ª–∞—Å—Å={{'–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∞—è' if pred == 1 else '–ù–æ—Ä–º–∞–ª—å–Ω–∞—è'}}")
```

## –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è
1. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ (~78:1)
2. –ì–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - F2-score (—Ñ–æ–∫—É—Å –Ω–∞ Recall - –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –º–æ—à–µ–Ω–Ω–∏–∫–∞)
3. –ü–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F2-score
4. –î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ SHAP –∑–Ω–∞—á–µ–Ω–∏—è (—Å–º. –≥—Ä–∞—Ñ–∏–∫–∏ –≤ model/)
"""

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
with open('model/MODEL_DOCUMENTATION.md', 'w', encoding='utf-8') as f:
    f.write(model_doc)

print("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ model/MODEL_DOCUMENTATION.md")

# ============================================================================
# –ò–¢–û–ì–ò
# ============================================================================
print("\n" + "="*80)
print("‚úÖ –í–°–ï –ó–ê–î–ê–ß–ò –í–´–ü–û–õ–ù–ï–ù–´!")
print("="*80)
print("\n–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏:")
print("  ‚úÖ Baseline (37-40): Train/test split, Logistic Regression, –º–µ—Ç—Ä–∏–∫–∏, confusion matrix")
print("  ‚úÖ Model Development (41-46): –û–±—É—á–µ–Ω—ã –∏ —Å—Ä–∞–≤–Ω–µ–Ω—ã RandomForest, XGBoost, LightGBM, CatBoost")
print("  ‚úÖ Optimization (47-51): –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, feature selection, CV, threshold optimization")
print("  ‚úÖ Interpretability (52-56): SHAP –∞–Ω–∞–ª–∏–∑, –≥—Ä–∞—Ñ–∏–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏–º–µ—Ä—ã –æ–±—ä—è—Å–Ω–µ–Ω–∏–π")
print("  ‚úÖ Model Packaging (57-59): –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")
print("\nüèÜ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: LightGBM —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
print(f"üìä F2-score: {final_metrics['F2-score']:.4f}")
print(f"üìä Recall: {final_metrics['Recall']:.4f}")
print(f"üìä Precision: {final_metrics['Precision']:.4f}")
print("\n‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ API!")

